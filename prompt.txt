Good question. The current setup is solid but **not optimal for competition**. Here are the gaps and fixes:

---

### Current Weaknesses

| Issue | Impact |
|-------|--------|
| Same learning rate for both agents | A-Agent needs precision, Q-Agent needs creativity — different optima |
| No evaluation split | Can't detect overfitting during training |
| `lora_alpha = r` (128=128) | Scaling factor of 1.0 is aggressive, can destabilize training |
| No system prompt in training data | But agents use system prompts at inference — train/test mismatch |
| Single-pass training | No iterative refinement (train A-Agent → test → retrain on failures) |
| `save_method="merged_16bit"` | Saves 28GB model; could save LoRA adapter only (~500MB) for faster iteration |
| No data augmentation | A-Agent sees each question once; shuffling answer positions would 3x effective data |

---

### Updated Training Cells

Replace cells 15-18 (A-Agent) and 21-24 (Q-Agent) with these optimized versions:

#### A-Agent: Load + LoRA (cell 15)

````python
# ========== LOAD QWEN2.5-14B FOR A-AGENT FINE-TUNING ==========
max_seq_length = 2048
dtype = torch.bfloat16
load_in_4bit = False

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="./hf_models/Qwen2.5-14B-Instruct",
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=load_in_4bit,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
)
print("Qwen2.5-14B loaded.")

# LoRA — alpha = 2*r for stable scaling (effective lr multiplier = alpha/r = 2)
model = FastLanguageModel.get_peft_model(
    model,
    r=128,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                    "gate_proj", "up_proj", "down_proj"],
    lora_alpha=256,        # 2x rank — standard best practice
    lora_dropout=0.05,     # Slight dropout prevents overfitting on 800 examples
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=3407,
)
print("LoRA adapters added (r=128, alpha=256).")
````

#### A-Agent: Prepare Data with System Prompt + Answer Augmentation (cell 16)

````python
# ========== PREPARE DATASET WITH CHAT TEMPLATE + AUGMENTATION ==========
tokenizer = get_chat_template(tokenizer, chat_template="qwen-2.5")

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

A_AGENT_SYSTEM = (
    "You are an expert competitive exam solver specializing in Logical Reasoning, "
    "Puzzles, Blood Relations, and Series Patterns. Analyze each question step by step. "
    "Output ONLY a JSON object: {\"answer\": \"<A/B/C/D>\", \"reasoning\": \"<brief>\"}"
)

def augment_a_agent_data(data):
    """Augment by shuffling answer choices — 2x more training data with position invariance."""
    augmented = []
    for item in data:
        # Original
        augmented.append(item)
        
        # Shuffled version — swap answer positions so model doesn't learn position bias
        try:
            convo = item["conversations"]
            user_msg = convo[0]["content"]
            asst_msg = json.loads(convo[1]["content"])
            
            # Parse choices from user message
            if "Choices:" in user_msg:
                parts = user_msg.split("Choices:")
                q_part = parts[0]
                choices_and_rest = parts[1].split("\n\n", 1)
                choices_str = choices_and_rest[0].strip()
                rest = choices_and_rest[1] if len(choices_and_rest) > 1 else ""
                
                # Extract individual choices
                import re
                choice_pattern = re.findall(r'([A-D])\)\s*(.+?)(?=\s+[A-D]\)|$)', choices_str)
                if len(choice_pattern) == 4:
                    labels = ["A", "B", "C", "D"]
                    original_answer = asst_msg["answer"]
                    original_idx = labels.index(original_answer)
                    
                    # Create one shuffled permutation
                    indices = list(range(4))
                    random.shuffle(indices)
                    new_choices = [f"{labels[i]}) {choice_pattern[indices[i]][1]}" for i in range(4)]
                    new_answer_idx = indices.index(original_idx)
                    new_answer = labels[new_answer_idx]
                    
                    new_choices_str = " ".join(new_choices)
                    new_user = f"{q_part}Choices: {new_choices_str}\n\n{rest}"
                    new_asst = json.dumps({"answer": new_answer, "reasoning": asst_msg["reasoning"]})
                    
                    augmented.append({"conversations": [
                        {"role": "user", "content": new_user},
                        {"role": "assistant", "content": new_asst}
                    ]})
        except (json.JSONDecodeError, KeyError, ValueError, IndexError):
            pass  # Skip failed augmentation
    
    return augmented

# Add system prompt to all conversations
def add_system_prompt(data, system_prompt):
    for item in data:
        item["conversations"].insert(0, {"role": "system", "content": system_prompt})
    return data

# Augment + add system prompt
a_data_aug = augment_a_agent_data(a_data)
a_data_aug = add_system_prompt(a_data_aug, A_AGENT_SYSTEM)
random.shuffle(a_data_aug)

# Split: 95% train, 5% eval
split_idx = int(len(a_data_aug) * 0.95)
a_train_data = a_data_aug[:split_idx]
a_eval_data = a_data_aug[split_idx:]

print(f"Original: {len(a_data)} | Augmented: {len(a_data_aug)} | Train: {len(a_train_data)} | Eval: {len(a_eval_data)}")

def formatting_prompts_func(examples):
    convos = examples["conversations"]
    texts = []
    for convo in convos:
        if isinstance(convo, list) and all(isinstance(msg, dict) for msg in convo):
            text = tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)
            texts.append(text)
    return {"text": texts}

a_train_dataset = Dataset.from_list(a_train_data)
a_eval_dataset = Dataset.from_list(a_eval_data)

a_train_dataset = standardize_sharegpt(a_train_dataset)
a_train_dataset = a_train_dataset.map(formatting_prompts_func, batched=True, remove_columns=a_train_dataset.column_names)
a_train_dataset = a_train_dataset.filter(lambda x: len(x["text"].strip()) > 0)

a_eval_dataset = standardize_sharegpt(a_eval_dataset)
a_eval_dataset = a_eval_dataset.map(formatting_prompts_func, batched=True, remove_columns=a_eval_dataset.column_names)
a_eval_dataset = a_eval_dataset.filter(lambda x: len(x["text"].strip()) > 0)

print(f"Train: {len(a_train_dataset)} | Eval: {len(a_eval_dataset)}")
if len(a_train_dataset) > 0:
    print(f"Sample: {a_train_dataset['text'][0][:400]}...")
````

#### A-Agent: Train with Eval (cell 17)

````python
# ========== TRAIN A-AGENT ==========
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=a_train_dataset,
    eval_dataset=a_eval_dataset,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),
    packing=False,
    args=SFTConfig(
        per_device_train_batch_size=16,
        gradient_accumulation_steps=4,
        warmup_ratio=0.05,               # 5% warmup (scales with data size)
        num_train_epochs=3,
        learning_rate=1e-4,              # Lower LR for A-Agent — precision matters
        logging_steps=5,
        eval_strategy="steps",
        eval_steps=50,                    # Eval every 50 steps — catch overfitting
        save_strategy="steps",
        save_steps=50,
        save_total_limit=2,
        load_best_model_at_end=True,      # Auto-select best checkpoint
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="cosine",
        seed=3407,
        output_dir="a_agent_training_output",
        report_to="none",
        bf16=True,
        dataloader_pin_memory=False,
        remove_unused_columns=True,
        gradient_checkpointing=True,
        dataloader_num_workers=0,
    ),
)

trainer = train_on_responses_only(
    trainer,
    instruction_part="<|im_start|>user\n",
    response_part="<|im_start|>assistant\n",
)

FastLanguageModel.for_training(model)
print("Starting A-Agent training (with eval)...")
trainer_stats = trainer.train()
print(f"A-Agent training complete!")
print(f"  Train loss: {trainer_stats.training_loss:.4f}")
# Print best eval loss
best_metric = trainer.state.best_metric
if best_metric is not None:
    print(f"  Best eval loss: {best_metric:.4f}")
````

#### A-Agent: Save (cell 18) — unchanged, keep as-is

---

#### Q-Agent: Prepare Data with System Prompt (cell 22)

````python
# ========== PREPARE Q-AGENT DATASET ==========
tokenizer = get_chat_template(tokenizer, chat_template="qwen-2.5")
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

Q_AGENT_SYSTEM = (
    "You are an expert question generator for competitive reasoning exams. "
    "Generate extremely difficult MCQs that are tricky but have exactly one correct answer. "
    "Output ONLY valid JSON with keys: topic, question, choices, answer, explanation."
)

# Add system prompt
for item in q_data:
    item["conversations"].insert(0, {"role": "system", "content": Q_AGENT_SYSTEM})

random.shuffle(q_data)
split_idx = int(len(q_data) * 0.95)
q_train_data = q_data[:split_idx]
q_eval_data = q_data[split_idx:]

q_train_dataset = Dataset.from_list(q_train_data)
q_eval_dataset = Dataset.from_list(q_eval_data)

q_train_dataset = standardize_sharegpt(q_train_dataset)
q_train_dataset = q_train_dataset.map(formatting_prompts_func, batched=True, remove_columns=q_train_dataset.column_names)
q_train_dataset = q_train_dataset.filter(lambda x: len(x["text"].strip()) > 0)

q_eval_dataset = standardize_sharegpt(q_eval_dataset)
q_eval_dataset = q_eval_dataset.map(formatting_prompts_func, batched=True, remove_columns=q_eval_dataset.column_names)
q_eval_dataset = q_eval_dataset.filter(lambda x: len(x["text"].strip()) > 0)

print(f"Q-Agent — Train: {len(q_train_dataset)} | Eval: {len(q_eval_dataset)}")
````

#### Q-Agent: Train with Eval (cell 23)

````python
# ========== TRAIN Q-AGENT ==========
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=q_train_dataset,
    eval_dataset=q_eval_dataset,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),
    packing=False,
    args=SFTConfig(
        per_device_train_batch_size=16,
        gradient_accumulation_steps=4,
        warmup_ratio=0.05,
        num_train_epochs=3,
        learning_rate=2e-4,              # Higher LR for Q-Agent — needs more creativity
        logging_steps=5,
        eval_strategy="steps",
        eval_steps=50,
        save_strategy="steps",
        save_steps=50,
        save_total_limit=2,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="cosine",
        seed=3407,
        output_dir="q_agent_training_output",
        report_to="none",
        bf16=True,
        dataloader_pin_memory=False,
        remove_unused_columns=True,
        gradient_checkpointing=True,
        dataloader_num_workers=0,
    ),
)

trainer = train_on_responses_only(
    trainer,
    instruction_part="<|im_start|>user\n",
    response_part="<|im_start|>assistant\n",
)

FastLanguageModel.for_training(model)
print("Starting Q-Agent training (with eval)...")
trainer_stats = trainer.train()
print(f"Q-Agent training complete!")
print(f"  Train loss: {trainer_stats.training_loss:.4f}")
best_metric = trainer.state.best_metric
if best_metric is not None:
    print(f"  Best eval loss: {best_metric:.4f}")
````

---

### Summary of Improvements

| Technique | Before | After |
|-----------|--------|-------|
| **LoRA alpha** | 128 (= r) | **256 (= 2r)** — standard scaling |
| **LoRA dropout** | 0 | **0.05** — regularization for small dataset |
| **System prompt** | Missing in training | **Added** — matches inference behavior |
| **Data augmentation** | None | **Answer position shuffling** — 2x A-Agent data |
| **Eval split** | None | **5% held out** — detects overfitting |
| **Best checkpoint** | Last checkpoint | **`load_best_model_at_end=True`** — auto-selects best |
| **A-Agent LR** | 2e-4 | **1e-4** — precision over speed |
| **Q-Agent LR** | 2e-4 | **2e-4** — keeps creativity |
| **Warmup** | Fixed 10 steps | **5% ratio** — scales with dataset |

These changes should meaningfully improve competition performance. Update the notebook and run from the top.