{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fd5c6c5",
   "metadata": {},
   "source": [
    "# AAIPL Fine-Tuning Pipeline ‚Äî Qwen3-4B\n",
    "\n",
    "**Target: Win the league on MI300X (192GB). ~30 min total.**\n",
    "\n",
    "| Phase | Time |\n",
    "|-------|------|\n",
    "| 0. Copy model | 2 min |\n",
    "| 1. Generate 400 MCQs (100/topic) | 12 min |\n",
    "| 2. Fine-tune A-Agent | 5 min |\n",
    "| 3. Fine-tune Q-Agent | 5 min |\n",
    "| 4. Test + Push | 5 min |\n",
    "\n",
    "**Strategy:**\n",
    "- **Adaptive verification** ‚Äî 2-way verify for Seating/Family/Series (works well); skip for Syllogisms (model can't self-solve)\n",
    "- **Answer hint rotation** ‚Äî balanced A/B/C/D\n",
    "- **Simple Syllogisms prompt** ‚Äî high JSON success rate\n",
    "- **Robust JSON extraction** ‚Äî multi-strategy parsing + auto-fix\n",
    "\n",
    "**Constraints:** Q-Agent <13s, A-Agent <9s, ‚â•50% filter pass rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c992ea6d",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 0: Setup ‚Äî Copy the Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf8bac30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1cfa9a7208912126459214e8b04321603b3df60c\n"
     ]
    }
   ],
   "source": [
    "# Find the Qwen3-4B snapshot hash\n",
    "!ls /root/.cache/huggingface/models--Qwen--Qwen3-4B/snapshots/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb6adb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE\t\t\t\t  model-00002-of-00003.safetensors\n",
      "README.md\t\t\t  model-00003-of-00003.safetensors\n",
      "config.json\t\t\t  model.safetensors.index.json\n",
      "generation_config.json\t\t  tokenizer.json\n",
      "merges.txt\t\t\t  tokenizer_config.json\n",
      "model-00001-of-00003.safetensors  vocab.json\n"
     ]
    }
   ],
   "source": [
    "# Copy the base model to hf_models/ (dereference symlinks with -L)\n",
    "!mkdir -p ./hf_models/Qwen3-4B\n",
    "!cp -rL /root/.cache/huggingface/models--Qwen--Qwen3-4B/snapshots/*/. ./hf_models/Qwen3-4B/\n",
    "!ls ./hf_models/Qwen3-4B/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4312183a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "#### Unsloth: `hf_xet==1.1.10` and `ipykernel>6.30.1` breaks progress bars. Disabling for now in XET.\n",
      "#### Unsloth: To re-enable progress bars, please downgrade to `ipykernel==6.30.1` or wait for a fix to\n",
      "https://github.com/huggingface/xet-core/issues/526\n",
      "INFO 02-15 08:35:46 [__init__.py:225] Automatically detected platform rocm.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "==((====))==  Unsloth 2025.10.9: Fast Qwen3 patching. Transformers: 4.56.2. vLLM: 0.11.1rc3.dev39+gf417746ad.rocm700.\n",
      "   \\\\   /|    . Num GPUs = 1. Max memory: 255.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0a0+git1c57644. ROCm Toolkit: 7.0.51831-a3e329ad8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-15 08:35:50] INFO modeling.py:987: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15cb261e2deb49679c23a51b49fe2942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ./hf_models/Qwen3-4B\n",
      "Parameters: 4.0B\n",
      "Base model verified and unloaded.\n"
     ]
    }
   ],
   "source": [
    "# Quick sanity check ‚Äî load and verify the base model\n",
    "from unsloth import FastLanguageModel\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"./hf_models/Qwen3-4B\",\n",
    "    max_seq_length=1024,\n",
    "    dtype=None,\n",
    "    load_in_4bit=False,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "print(f\"Model: {model.config._name_or_path}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()) / 1e9:.1f}B\")\n",
    "del model, tokenizer\n",
    "import gc, torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Base model verified and unloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9279bd",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 1: Generate Synthetic Training Data\n",
    "\n",
    "Qwen3-4B as teacher ‚Üí **400 MCQs** (100/topic).\n",
    "\n",
    "- `enable_thinking=False` to prevent `<think>` tags\n",
    "- **Adaptive verification**: 2-way verify for Seating/Family/Series, skip for Syllogisms\n",
    "- Answer hint rotation for balanced A/B/C/D distribution\n",
    "- Robust JSON extraction (markdown blocks, brace matching, auto-fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53f3fd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.10.9: Fast Qwen3 patching. Transformers: 4.56.2. vLLM: 0.11.1rc3.dev39+gf417746ad.rocm700.\n",
      "   \\\\   /|    . Num GPUs = 1. Max memory: 255.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0a0+git1c57644. ROCm Toolkit: 7.0.51831-a3e329ad8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-15 08:36:01] INFO modeling.py:987: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e97073d634a041b4a75fefb8bed2ae4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/unsloth/kernels/utils.py:743: UserWarning: An output with one or more elements was resized since it had shape [1, 32, 2560], which does not match the required output shape [32, 1, 2560]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /app/pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  out = torch_matmul(X, W.t(), out = out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 32 in 9.6s (0.30s each)\n",
      "Verify topics: {'Family tree logic', 'Mixed Series (Alphanumeric)', 'Seating Arrangements (Linear, Circular)'}\n",
      "Skip verify: {'Syllogisms'}\n",
      "GPU: 10.2 GiB\n"
     ]
    }
   ],
   "source": [
    "# ========== LOAD QWEN3-4B AS TEACHER + ADAPTIVE VERIFICATION ==========\n",
    "import json, time, random, re, gc, torch\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "TEACHER_PATH = \"./hf_models/Qwen3-4B\"\n",
    "\n",
    "teacher_model, teacher_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=TEACHER_PATH,\n",
    "    max_seq_length=2048,\n",
    "    dtype=torch.bfloat16,\n",
    "    load_in_4bit=False,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "FastLanguageModel.for_inference(teacher_model)\n",
    "\n",
    "if teacher_tokenizer.pad_token is None:\n",
    "    teacher_tokenizer.pad_token = teacher_tokenizer.eos_token\n",
    "    teacher_tokenizer.pad_token_id = teacher_tokenizer.eos_token_id\n",
    "teacher_tokenizer.padding_side = \"left\"\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def query_teacher_batch(system_prompts, user_prompts, temperature=0.7, max_tokens=512):\n",
    "    \"\"\"Batched inference with Qwen3 <think> tag handling.\"\"\"\n",
    "    messages_list = []\n",
    "    for sys_p, usr_p in zip(system_prompts, user_prompts):\n",
    "        messages_list.append([\n",
    "            {\"role\": \"system\", \"content\": sys_p},\n",
    "            {\"role\": \"user\", \"content\": usr_p}\n",
    "        ])\n",
    "    \n",
    "    texts = [teacher_tokenizer.apply_chat_template(\n",
    "        m, tokenize=False, add_generation_prompt=True,\n",
    "        enable_thinking=False\n",
    "    ) for m in messages_list]\n",
    "    \n",
    "    inputs = teacher_tokenizer(\n",
    "        texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=1536\n",
    "    ).to(teacher_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = teacher_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=teacher_tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    responses = []\n",
    "    for output in outputs:\n",
    "        raw = teacher_tokenizer.decode(output[input_len:], skip_special_tokens=True)\n",
    "        raw = re.sub(r'<think>.*?</think>', '', raw, flags=re.DOTALL).strip()\n",
    "        responses.append(raw)\n",
    "    return responses\n",
    "\n",
    "def verify_answer(question_text: str, choices: list, num_rounds: int = 2) -> tuple:\n",
    "    \"\"\"Ask the teacher to solve an MCQ multiple times. Returns (majority_answer, confidence, reasoning).\n",
    "    Returns (None, 0.0, '') if no majority.\"\"\"\n",
    "    choices_str = \" \".join(choices)\n",
    "    sys_p = \"You are an expert. Answer the MCQ. Output ONLY JSON: {\\\"answer\\\": \\\"A/B/C/D\\\", \\\"reasoning\\\": \\\"brief\\\"}\"\n",
    "    usr_p = f\"Question: {question_text}\\nChoices: {choices_str}\\n\\nOutput JSON only.\"\n",
    "    \n",
    "    responses = query_teacher_batch([sys_p] * num_rounds, [usr_p] * num_rounds, temperature=0.3, max_tokens=150)\n",
    "    \n",
    "    answers = []\n",
    "    reasonings = []\n",
    "    for raw in responses:\n",
    "        raw = re.sub(r'<think>.*?</think>', '', raw, flags=re.DOTALL).strip()\n",
    "        try:\n",
    "            start = raw.find('{')\n",
    "            end = raw.rfind('}') + 1\n",
    "            if start >= 0 and end > start:\n",
    "                parsed = json.loads(raw[start:end])\n",
    "                if \"answer\" in parsed:\n",
    "                    ans = str(parsed[\"answer\"]).strip()[0].upper()\n",
    "                    if ans in \"ABCD\":\n",
    "                        answers.append(ans)\n",
    "                        reasonings.append(str(parsed.get(\"reasoning\", \"\")))\n",
    "        except (json.JSONDecodeError, IndexError, KeyError):\n",
    "            pass\n",
    "    \n",
    "    if not answers:\n",
    "        return None, 0.0, \"\"\n",
    "    \n",
    "    counts = Counter(answers)\n",
    "    majority_ans, majority_count = counts.most_common(1)[0]\n",
    "    confidence = majority_count / len(answers)\n",
    "    \n",
    "    best_reasoning = \"\"\n",
    "    for a, r in zip(answers, reasonings):\n",
    "        if a == majority_ans and len(r) > len(best_reasoning):\n",
    "            best_reasoning = r\n",
    "    \n",
    "    return majority_ans, confidence, best_reasoning\n",
    "\n",
    "# Topics that should use verification (model CAN self-solve these)\n",
    "VERIFY_TOPICS = {\"Seating Arrangements (Linear, Circular)\", \"Family tree logic\", \"Mixed Series (Alphanumeric)\"}\n",
    "# Syllogisms: NO verification (model always defaults to \"D\")\n",
    "SKIP_VERIFY_TOPICS = {\"Syllogisms\"}\n",
    "\n",
    "# Quick test\n",
    "t0 = time.time()\n",
    "batch_test = query_teacher_batch(\n",
    "    [\"You are helpful.\"] * BATCH_SIZE,\n",
    "    [f\"What is {i+1} + {i+1}?\" for i in range(BATCH_SIZE)],\n",
    "    max_tokens=20\n",
    ")\n",
    "t1 = time.time()\n",
    "print(f\"Batch {BATCH_SIZE} in {t1-t0:.1f}s ({(t1-t0)/BATCH_SIZE:.2f}s each)\")\n",
    "print(f\"Verify topics: {VERIFY_TOPICS}\")\n",
    "print(f\"Skip verify: {SKIP_VERIFY_TOPICS}\")\n",
    "print(f\"GPU: {torch.cuda.memory_allocated()/1024**3:.1f} GiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9d4c6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total to generate: 200\n",
      "  Syllogisms: 50 | verify=False\n",
      "  Seating Arrangements (Linear, Circular): 50 | verify=True\n",
      "  Family tree logic: 50 | verify=True\n",
      "  Mixed Series (Alphanumeric): 50 | verify=True\n"
     ]
    }
   ],
   "source": [
    "# ========== TOPICS CONFIG (100 per topic = 400 total) ==========\n",
    "\n",
    "QUESTIONS_PER_TOPIC = 50\n",
    "\n",
    "TOPICS_CONFIG = {\n",
    "    \"Syllogisms\": {\n",
    "        \"count\": QUESTIONS_PER_TOPIC,\n",
    "        \"parent\": \"Logical Reasoning\",\n",
    "        \"verify\": False,  # Model can't self-solve syllogisms ‚Äî always says D\n",
    "        \"system\": \"You create syllogism MCQ problems. Output ONLY valid JSON, no other text.\",\n",
    "        \"prompt_template\": \"\"\"Create a syllogism MCQ with {num_statements} statements and {num_conclusions} conclusions.\n",
    "Use quantifiers: All, Some, No, Some...not.\n",
    "The correct answer MUST be \"{answer_hint}\".\n",
    "\n",
    "Output ONLY this JSON:\n",
    "{{\"topic\": \"Logical Reasoning/Syllogisms\", \"question\": \"Statement I: ...\\\\nStatement II: ...\\\\nConclusion I: ...\\\\nConclusion II: ...\", \"choices\": [\"A) Only conclusion I follows\", \"B) Only conclusion II follows\", \"C) Both I and II follow\", \"D) Neither I nor II follows\"], \"answer\": \"{answer_hint}\", \"explanation\": \"brief reason\"}}\"\"\"\n",
    "    },\n",
    "    \"Seating Arrangements (Linear, Circular)\": {\n",
    "        \"count\": QUESTIONS_PER_TOPIC,\n",
    "        \"parent\": \"Puzzles\",\n",
    "        \"verify\": True,  # 2-way verification works well here\n",
    "        \"system\": \"You create seating arrangement MCQ puzzles. Output ONLY valid JSON, no other text.\",\n",
    "        \"prompt_template\": \"\"\"Create a {arrangement_type} seating arrangement MCQ with {num_people} people.\n",
    "Include positional constraints and facing directions. The correct answer is \"{answer_hint}\".\n",
    "\n",
    "Output ONLY this JSON:\n",
    "{{\"topic\": \"Puzzles/Seating Arrangements (Linear, Circular)\", \"question\": \"full question with constraints\", \"choices\": [\"A) option1\", \"B) option2\", \"C) option3\", \"D) option4\"], \"answer\": \"{answer_hint}\", \"explanation\": \"step-by-step deduction\"}}\"\"\"\n",
    "    },\n",
    "    \"Family tree logic\": {\n",
    "        \"count\": QUESTIONS_PER_TOPIC,\n",
    "        \"parent\": \"Blood Relations and Family Tree\",\n",
    "        \"verify\": True,\n",
    "        \"system\": \"You create blood relation MCQ puzzles. Output ONLY valid JSON, no other text.\",\n",
    "        \"prompt_template\": \"\"\"Create a blood relation MCQ with a chain of {num_relations} family relationships.\n",
    "Use indirect descriptions. The correct answer is \"{answer_hint}\".\n",
    "\n",
    "Output ONLY this JSON:\n",
    "{{\"topic\": \"Blood Relations and Family Tree/Family tree logic\", \"question\": \"full question\", \"choices\": [\"A) relation1\", \"B) relation2\", \"C) relation3\", \"D) relation4\"], \"answer\": \"{answer_hint}\", \"explanation\": \"step-by-step chain\"}}\"\"\"\n",
    "    },\n",
    "    \"Mixed Series (Alphanumeric)\": {\n",
    "        \"count\": QUESTIONS_PER_TOPIC,\n",
    "        \"parent\": \"Series and Patterns\",\n",
    "        \"verify\": True,\n",
    "        \"system\": \"You create number/letter series MCQ problems. Output ONLY valid JSON, no other text.\",\n",
    "        \"prompt_template\": \"\"\"Create a {series_type} series MCQ with {num_elements} elements using a {pattern_type} pattern.\n",
    "The correct answer is \"{answer_hint}\".\n",
    "\n",
    "Output ONLY this JSON:\n",
    "{{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"series question\", \"choices\": [\"A) opt1\", \"B) opt2\", \"C) opt3\", \"D) opt4\"], \"answer\": \"{answer_hint}\", \"explanation\": \"pattern explanation\"}}\"\"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Answer distribution tracker\n",
    "answer_counters = {topic: {\"A\": 0, \"B\": 0, \"C\": 0, \"D\": 0} for topic in TOPICS_CONFIG}\n",
    "\n",
    "def get_answer_hint(topic: str) -> str:\n",
    "    \"\"\"Return the least-used answer letter for balanced distribution.\"\"\"\n",
    "    counts = answer_counters[topic]\n",
    "    min_count = min(counts.values())\n",
    "    least_used = [l for l, c in counts.items() if c == min_count]\n",
    "    return random.choice(least_used)\n",
    "\n",
    "def randomize_params(topic):\n",
    "    params = {\"answer_hint\": get_answer_hint(topic)}\n",
    "    if topic == \"Syllogisms\":\n",
    "        params.update({\"num_statements\": random.choice([2, 3]), \"num_conclusions\": random.choice([2, 3])})\n",
    "    elif topic == \"Seating Arrangements (Linear, Circular)\":\n",
    "        params.update({\"arrangement_type\": random.choice([\"linear\", \"circular\"]), \"num_people\": random.choice([5, 6, 7, 8])})\n",
    "    elif topic == \"Family tree logic\":\n",
    "        params.update({\"num_relations\": random.choice([3, 4, 5, 6])})\n",
    "    elif topic == \"Mixed Series (Alphanumeric)\":\n",
    "        params.update({\n",
    "            \"series_type\": random.choice([\"alphanumeric\", \"number\", \"mixed\"]),\n",
    "            \"num_elements\": random.choice([5, 6, 7]),\n",
    "            \"pattern_type\": random.choice([\"arithmetic\", \"alternating\", \"geometric\", \"fibonacci\"])\n",
    "        })\n",
    "    return params\n",
    "\n",
    "print(f\"Total to generate: {sum(t['count'] for t in TOPICS_CONFIG.values())}\")\n",
    "for t, c in TOPICS_CONFIG.items():\n",
    "    print(f\"  {t}: {c['count']} | verify={c['verify']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42c058e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Generating 50 for: Syllogisms [DIRECT (trust hint)]\n",
      "============================================================\n",
      "  [Syllogisms] 9/50 | 188 q/min | fails: 23 | A/B/C/D: 3/2/1/3\n",
      "  [Syllogisms] 9/50 | 93 q/min | fails: 55 | A/B/C/D: 3/2/1/3\n",
      "  [Syllogisms] 11/50 | 75 q/min | fails: 85 | A/B/C/D: 3/2/3/3\n",
      "  [Syllogisms] 15/50 | 75 q/min | fails: 113 | A/B/C/D: 3/6/3/3\n",
      "  [Syllogisms] 17/50 | 68 q/min | fails: 143 | A/B/C/D: 3/6/3/5\n",
      "  [Syllogisms] 17/50 | 57 q/min | fails: 175 | A/B/C/D: 3/6/3/5\n",
      "  [Syllogisms] 17/50 | 49 q/min | fails: 207 | A/B/C/D: 3/6/3/5\n",
      "  [Syllogisms] 18/50 | 45 q/min | fails: 238 | A/B/C/D: 4/6/3/5\n",
      "  [Syllogisms] 18/50 | 40 q/min | fails: 270 | A/B/C/D: 4/6/3/5\n",
      "  [Syllogisms] 18/50 | 36 q/min | fails: 302 | A/B/C/D: 4/6/3/5\n",
      "  DONE: 18 / 320 attempts (302 fails)\n",
      "\n",
      "============================================================\n",
      "Generating 50 for: Seating Arrangements (Linear, Circular) [2-way VERIFY]\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/unsloth/kernels/utils.py:743: UserWarning: An output with one or more elements was resized since it had shape [1, 2, 2560], which does not match the required output shape [2, 1, 2560]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /app/pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  out = torch_matmul(X, W.t(), out = out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Seating Arrangements (Linear, Circular)] 22/50 | 29 q/min | fails: 10 | A/B/C/D: 0/5/7/10\n",
      "  [Seating Arrangements (Linear, Circular)] 43/50 | 27 q/min | fails: 21 | A/B/C/D: 3/11/10/19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/unsloth/kernels/utils.py:743: UserWarning: An output with one or more elements was resized since it had shape [1, 15, 2560], which does not match the required output shape [15, 1, 2560]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /app/pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  out = torch_matmul(X, W.t(), out = out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Seating Arrangements (Linear, Circular)] 50/50 | 26 q/min | fails: 24 | A/B/C/D: 3/13/10/24\n",
      "  DONE: 50 / 79 attempts (24 fails)\n",
      "\n",
      "============================================================\n",
      "Generating 50 for: Family tree logic [2-way VERIFY]\n",
      "============================================================\n",
      "  [Family tree logic] 19/50 | 28 q/min | fails: 13 | A/B/C/D: 8/7/2/2\n",
      "  [Family tree logic] 28/50 | 29 q/min | fails: 36 | A/B/C/D: 11/11/4/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/unsloth/kernels/utils.py:743: UserWarning: An output with one or more elements was resized since it had shape [1, 30, 2560], which does not match the required output shape [30, 1, 2560]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /app/pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  out = torch_matmul(X, W.t(), out = out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Family tree logic] 41/50 | 30 q/min | fails: 53 | A/B/C/D: 12/16/7/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/unsloth/kernels/utils.py:743: UserWarning: An output with one or more elements was resized since it had shape [1, 17, 2560], which does not match the required output shape [17, 1, 2560]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /app/pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  out = torch_matmul(X, W.t(), out = out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Family tree logic] 47/50 | 29 q/min | fails: 64 | A/B/C/D: 12/19/7/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/unsloth/kernels/utils.py:743: UserWarning: An output with one or more elements was resized since it had shape [1, 11, 2560], which does not match the required output shape [11, 1, 2560]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /app/pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  out = torch_matmul(X, W.t(), out = out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Family tree logic] 50/50 | 29 q/min | fails: 66 | A/B/C/D: 13/19/9/9\n",
      "  DONE: 50 / 122 attempts (66 fails)\n",
      "\n",
      "============================================================\n",
      "Generating 50 for: Mixed Series (Alphanumeric) [2-way VERIFY]\n",
      "============================================================\n",
      "  [Mixed Series (Alphanumeric)] 13/50 | 30 q/min | fails: 19 | A/B/C/D: 12/0/1/0\n",
      "  [Mixed Series (Alphanumeric)] 20/50 | 31 q/min | fails: 44 | A/B/C/D: 17/2/1/0\n",
      "  [Mixed Series (Alphanumeric)] 23/50 | 30 q/min | fails: 73 | A/B/C/D: 19/3/1/0\n",
      "  [Mixed Series (Alphanumeric)] 24/50 | 30 q/min | fails: 104 | A/B/C/D: 20/3/1/0\n",
      "  [Mixed Series (Alphanumeric)] 25/50 | 30 q/min | fails: 135 | A/B/C/D: 21/3/1/0\n",
      "  [Mixed Series (Alphanumeric)] 28/50 | 30 q/min | fails: 164 | A/B/C/D: 23/4/1/0\n",
      "  [Mixed Series (Alphanumeric)] 29/50 | 29 q/min | fails: 193 | A/B/C/D: 24/4/1/0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/unsloth/kernels/utils.py:743: UserWarning: An output with one or more elements was resized since it had shape [1, 29, 2560], which does not match the required output shape [29, 1, 2560]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /app/pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  out = torch_matmul(X, W.t(), out = out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Mixed Series (Alphanumeric)] 31/50 | 29 q/min | fails: 220 | A/B/C/D: 26/4/1/0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/unsloth/kernels/utils.py:743: UserWarning: An output with one or more elements was resized since it had shape [1, 27, 2560], which does not match the required output shape [27, 1, 2560]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /app/pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  out = torch_matmul(X, W.t(), out = out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Mixed Series (Alphanumeric)] 32/50 | 29 q/min | fails: 246 | A/B/C/D: 27/4/1/0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/unsloth/kernels/utils.py:743: UserWarning: An output with one or more elements was resized since it had shape [1, 26, 2560], which does not match the required output shape [26, 1, 2560]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /app/pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  out = torch_matmul(X, W.t(), out = out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Mixed Series (Alphanumeric)] 32/50 | 28 q/min | fails: 272 | A/B/C/D: 27/4/1/0\n",
      "  DONE: 32 / 304 attempts (272 fails)\n",
      "\n",
      "============================================================\n",
      "DONE in 5.3 min | Total: 150\n",
      "Verified (Seating/Family/Series): 127\n",
      "Direct accept (Syllogisms): 18\n",
      "Low-conf fallback: 5\n",
      "Answer distribution:\n",
      "  A: 47\n",
      "  B: 42\n",
      "  C: 23\n",
      "  D: 38\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ========== GENERATE TRAINING DATA ‚Äî ADAPTIVE VERIFICATION ==========\n",
    "Path(\"training_data\").mkdir(exist_ok=True)\n",
    "all_a_agent_data = []\n",
    "all_q_agent_data = []\n",
    "\n",
    "# --- Robust JSON extraction ---\n",
    "def extract_json(raw: str) -> dict:\n",
    "    raw = re.sub(r'<think>.*?</think>', '', raw, flags=re.DOTALL).strip()\n",
    "    \n",
    "    if '```json' in raw:\n",
    "        try:\n",
    "            block = raw.split('```json')[1].split('```')[0].strip()\n",
    "            return json.loads(block)\n",
    "        except (json.JSONDecodeError, IndexError):\n",
    "            pass\n",
    "    if '```' in raw:\n",
    "        try:\n",
    "            block = raw.split('```')[1].split('```')[0].strip()\n",
    "            if block.startswith('json'):\n",
    "                block = block[4:].strip()\n",
    "            return json.loads(block)\n",
    "        except (json.JSONDecodeError, IndexError):\n",
    "            pass\n",
    "    \n",
    "    start = raw.find('{')\n",
    "    end = raw.rfind('}') + 1\n",
    "    if start >= 0 and end > start:\n",
    "        candidate = raw[start:end]\n",
    "        try:\n",
    "            return json.loads(candidate)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "        fixed = candidate.replace(\"'\", '\"')\n",
    "        fixed = re.sub(r',\\s*}', '}', fixed)\n",
    "        fixed = re.sub(r',\\s*]', ']', fixed)\n",
    "        try:\n",
    "            return json.loads(fixed)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "# --- Validation with auto-fix ---\n",
    "def validate_and_fix(parsed: dict) -> tuple:\n",
    "    if not isinstance(parsed, dict):\n",
    "        return False, \"Not a dict\"\n",
    "    for key in [\"question\", \"choices\", \"answer\"]:\n",
    "        if key not in parsed:\n",
    "            return False, f\"Missing: {key}\"\n",
    "    if not isinstance(parsed[\"choices\"], list) or len(parsed[\"choices\"]) != 4:\n",
    "        return False, \"Need 4 choices\"\n",
    "    \n",
    "    labels = [\"A\", \"B\", \"C\", \"D\"]\n",
    "    fixed = []\n",
    "    for i, c in enumerate(parsed[\"choices\"]):\n",
    "        if not isinstance(c, str) or len(c.strip()) < 1:\n",
    "            return False, f\"Empty choice {i}\"\n",
    "        c = c.strip()\n",
    "        if len(c) < 2 or c[1] != ')' or c[0].upper() not in \"ABCD\":\n",
    "            c = f\"{labels[i]}) {c}\"\n",
    "        if c[0].upper() != labels[i]:\n",
    "            text = c[3:].strip() if len(c) > 3 and c[1] == ')' else c\n",
    "            c = f\"{labels[i]}) {text}\"\n",
    "        fixed.append(c)\n",
    "    parsed[\"choices\"] = fixed\n",
    "    \n",
    "    texts = [c[3:].strip().lower() for c in fixed if len(c) > 3]\n",
    "    if len(set(texts)) < 3:\n",
    "        return False, \"Too many duplicate choices\"\n",
    "    \n",
    "    ans = str(parsed[\"answer\"]).strip()\n",
    "    if len(ans) >= 1 and ans[0].upper() in \"ABCD\":\n",
    "        parsed[\"answer\"] = ans[0].upper()\n",
    "    else:\n",
    "        return False, f\"Bad answer: {ans}\"\n",
    "    \n",
    "    if not isinstance(parsed[\"question\"], str) or len(parsed[\"question\"].strip()) < 10:\n",
    "        return False, \"Question too short\"\n",
    "    \n",
    "    if \"explanation\" not in parsed or not parsed.get(\"explanation\") or len(str(parsed.get(\"explanation\", \"\"))) < 3:\n",
    "        parsed[\"explanation\"] = \"Analyze systematically to find the correct answer.\"\n",
    "    \n",
    "    return True, \"OK\"\n",
    "\n",
    "# --- Dedup ---\n",
    "def is_unique(new_q: str, existing: list, threshold=0.8) -> bool:\n",
    "    words = set(new_q.lower().split())\n",
    "    if len(words) < 3:\n",
    "        return True\n",
    "    for eq in existing[-80:]:\n",
    "        ew = set(eq.lower().split())\n",
    "        if not ew: continue\n",
    "        overlap = len(words & ew) / max(len(words | ew), 1)\n",
    "        if overlap > threshold:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# ===== MAIN GENERATION LOOP =====\n",
    "existing_qs = []\n",
    "gen_start = time.time()\n",
    "verified_count = 0\n",
    "skipped_low_conf = 0\n",
    "direct_accept = 0\n",
    "\n",
    "for topic, config in TOPICS_CONFIG.items():\n",
    "    use_verify = config.get(\"verify\", False)\n",
    "    mode = \"2-way VERIFY\" if use_verify else \"DIRECT (trust hint)\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Generating {config['count']} for: {topic} [{mode}]\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    topic_data = []\n",
    "    fails = 0\n",
    "    max_attempts = config[\"count\"] * 6\n",
    "    attempts = 0\n",
    "    \n",
    "    while len(topic_data) < config[\"count\"] and attempts < max_attempts:\n",
    "        needed = min(BATCH_SIZE, config[\"count\"] - len(topic_data) + 8)\n",
    "        batch_sys = [config[\"system\"]] * needed\n",
    "        batch_usr = [config[\"prompt_template\"].format(**randomize_params(topic)) for _ in range(needed)]\n",
    "        \n",
    "        responses = query_teacher_batch(batch_sys, batch_usr, temperature=0.7, max_tokens=512)\n",
    "        attempts += len(responses)\n",
    "        \n",
    "        for raw in responses:\n",
    "            if len(topic_data) >= config[\"count\"]:\n",
    "                break\n",
    "            \n",
    "            parsed = extract_json(raw)\n",
    "            if parsed is None:\n",
    "                fails += 1\n",
    "                continue\n",
    "            \n",
    "            valid, reason = validate_and_fix(parsed)\n",
    "            if not valid:\n",
    "                fails += 1\n",
    "                continue\n",
    "            \n",
    "            if not is_unique(parsed[\"question\"], existing_qs):\n",
    "                fails += 1\n",
    "                continue\n",
    "            \n",
    "            # === ADAPTIVE VERIFICATION ===\n",
    "            if use_verify:\n",
    "                # 2-way verification for Seating/Family/Series\n",
    "                v_ans, v_conf, v_reasoning = verify_answer(parsed[\"question\"], parsed[\"choices\"], num_rounds=2)\n",
    "                \n",
    "                if v_ans is None or v_conf < 0.5:\n",
    "                    # Verification failed ‚Äî still accept with generator's answer (don't waste it)\n",
    "                    skipped_low_conf += 1\n",
    "                    # Keep generator's answer from hint\n",
    "                else:\n",
    "                    # Use verified answer + better reasoning\n",
    "                    parsed[\"answer\"] = v_ans\n",
    "                    if v_reasoning and len(v_reasoning) > len(str(parsed.get(\"explanation\", \"\"))):\n",
    "                        parsed[\"explanation\"] = v_reasoning\n",
    "                    verified_count += 1\n",
    "            else:\n",
    "                # Syllogisms: trust the generator's answer (= the hint we gave it)\n",
    "                direct_accept += 1\n",
    "            \n",
    "            existing_qs.append(parsed[\"question\"])\n",
    "            answer = parsed[\"answer\"]\n",
    "            explanation = str(parsed.get(\"explanation\", \"Solve step by step.\"))[:400]\n",
    "            answer_counters[topic][answer] += 1\n",
    "            \n",
    "            # A-Agent training example\n",
    "            choices_str = \" \".join(parsed[\"choices\"])\n",
    "            all_a_agent_data.append({\"conversations\": [\n",
    "                {\"role\": \"user\", \"content\": f\"Question: {parsed['question']}\\nChoices: {choices_str}\\n\\nSolve step by step and output JSON: {{\\\"answer\\\": \\\"<letter>\\\", \\\"reasoning\\\": \\\"<brief>\\\"}}\"},\n",
    "                {\"role\": \"assistant\", \"content\": json.dumps({\"answer\": answer, \"reasoning\": explanation})}\n",
    "            ]})\n",
    "            \n",
    "            # Q-Agent training example\n",
    "            full_topic = f\"{config['parent']}/{topic}\"\n",
    "            all_q_agent_data.append({\"conversations\": [\n",
    "                {\"role\": \"user\", \"content\": f\"Generate a difficult MCQ on topic: {full_topic}. Output ONLY valid JSON.\"},\n",
    "                {\"role\": \"assistant\", \"content\": json.dumps({\"topic\": full_topic, \"question\": parsed[\"question\"], \"choices\": parsed[\"choices\"], \"answer\": answer, \"explanation\": explanation})}\n",
    "            ]})\n",
    "            \n",
    "            topic_data.append(parsed)\n",
    "        \n",
    "        elapsed = time.time() - gen_start\n",
    "        rate = len(existing_qs) / elapsed * 60 if elapsed > 0 else 0\n",
    "        dist = answer_counters[topic]\n",
    "        dist_str = \"/\".join(f\"{dist[l]}\" for l in \"ABCD\")\n",
    "        print(f\"  [{topic}] {len(topic_data)}/{config['count']} | {rate:.0f} q/min | fails: {fails} | A/B/C/D: {dist_str}\")\n",
    "    \n",
    "    safe = topic.replace(' ', '_').replace('/', '_').replace('(', '').replace(')', '')\n",
    "    with open(f\"training_data/{safe}.json\", 'w') as f:\n",
    "        json.dump(topic_data, f, indent=2)\n",
    "    print(f\"  DONE: {len(topic_data)} / {attempts} attempts ({fails} fails)\")\n",
    "\n",
    "# Save combined\n",
    "with open(\"training_data/a_agent_train.json\", 'w') as f:\n",
    "    json.dump(all_a_agent_data, f, indent=2)\n",
    "with open(\"training_data/q_agent_train.json\", 'w') as f:\n",
    "    json.dump(all_q_agent_data, f, indent=2)\n",
    "\n",
    "total = time.time() - gen_start\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"DONE in {total/60:.1f} min | Total: {len(all_a_agent_data)}\")\n",
    "print(f\"Verified (Seating/Family/Series): {verified_count}\")\n",
    "print(f\"Direct accept (Syllogisms): {direct_accept}\")\n",
    "print(f\"Low-conf fallback: {skipped_low_conf}\")\n",
    "print(f\"Answer distribution:\")\n",
    "for label in \"ABCD\":\n",
    "    t = sum(answer_counters[tp][label] for tp in TOPICS_CONFIG)\n",
    "    print(f\"  {label}: {t}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e6b2ed",
   "metadata": {},
   "source": [
    "---\n",
    "## Validate Generated Data & Unload Teacher\n",
    "\n",
    "Quick sanity checks, then free GPU for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a832952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA VALIDATION REPORT\n",
      "============================================================\n",
      "\n",
      "a_agent_train.json: 150 examples\n",
      "  ‚úÖ All valid!\n",
      "\n",
      "q_agent_train.json: 150 examples\n",
      "  ‚úÖ All valid!\n",
      "\n",
      "Answer Distribution:\n",
      "  A:  47 (31.3%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  B:  42 (28.0%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  C:  23 (15.3%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  D:  38 (25.3%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "Per-Topic:\n",
      "  Syllogisms: 18 | A:4 | B:6 | C:3 | D:5\n",
      "  Seating: 50 | A:3 | B:13 | C:10 | D:24\n",
      "  Family: 50 | A:13 | B:19 | C:9 | D:9\n",
      "  Series: 32 | A:27 | B:4 | C:1 | D:0\n",
      "\n",
      "A/Q consistency: ‚úÖ All match\n",
      "============================================================\n",
      "GPU freed: 240.6 GiB available\n"
     ]
    }
   ],
   "source": [
    "# ========== VALIDATE DATA + UNLOAD TEACHER ==========\n",
    "import json\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA VALIDATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name in [\"a_agent_train.json\", \"q_agent_train.json\"]:\n",
    "    with open(f\"training_data/{name}\") as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"\\n{name}: {len(data)} examples\")\n",
    "    \n",
    "    errors = 0\n",
    "    for i, item in enumerate(data):\n",
    "        convos = item.get(\"conversations\", [])\n",
    "        if len(convos) != 2: errors += 1; continue\n",
    "        if convos[0][\"role\"] != \"user\" or convos[1][\"role\"] != \"assistant\": errors += 1; continue\n",
    "        try:\n",
    "            parsed = json.loads(convos[1][\"content\"])\n",
    "            if parsed.get(\"answer\") not in \"ABCD\": errors += 1\n",
    "        except json.JSONDecodeError:\n",
    "            errors += 1\n",
    "    \n",
    "    print(f\"  {'‚úÖ All valid!' if errors == 0 else f'‚ö†Ô∏è {errors} errors'}\")\n",
    "\n",
    "# Answer distribution\n",
    "print(\"\\nAnswer Distribution:\")\n",
    "with open(\"training_data/a_agent_train.json\") as f:\n",
    "    a_data = json.load(f)\n",
    "counts = {\"A\": 0, \"B\": 0, \"C\": 0, \"D\": 0}\n",
    "for item in a_data:\n",
    "    ans = json.loads(item[\"conversations\"][1][\"content\"]).get(\"answer\", \"?\")\n",
    "    if ans in counts: counts[ans] += 1\n",
    "total = sum(counts.values())\n",
    "for letter, count in counts.items():\n",
    "    pct = count / total * 100 if total > 0 else 0\n",
    "    bar = \"‚ñà\" * int(pct / 2)\n",
    "    print(f\"  {letter}: {count:3d} ({pct:4.1f}%) {bar}\")\n",
    "\n",
    "# Per-topic\n",
    "print(\"\\nPer-Topic:\")\n",
    "for label, fname in [(\"Syllogisms\", \"Syllogisms.json\"), (\"Seating\", \"Seating_Arrangements_Linear,_Circular.json\"),\n",
    "                     (\"Family\", \"Family_tree_logic.json\"), (\"Series\", \"Mixed_Series_Alphanumeric.json\")]:\n",
    "    path = Path(\"training_data\") / fname\n",
    "    if path.exists():\n",
    "        with open(path) as f: td = json.load(f)\n",
    "        tc = {\"A\": 0, \"B\": 0, \"C\": 0, \"D\": 0}\n",
    "        for q in td:\n",
    "            if q.get(\"answer\") in tc: tc[q[\"answer\"]] += 1\n",
    "        dist = \" | \".join(f\"{l}:{tc[l]}\" for l in \"ABCD\")\n",
    "        print(f\"  {label}: {len(td)} | {dist}\")\n",
    "    else:\n",
    "        print(f\"  {label}: MISSING\")\n",
    "\n",
    "# Cross-check\n",
    "with open(\"training_data/q_agent_train.json\") as f:\n",
    "    q_data = json.load(f)\n",
    "mismatches = sum(1 for a, q in zip(a_data, q_data)\n",
    "    if json.loads(a[\"conversations\"][1][\"content\"])[\"answer\"] != json.loads(q[\"conversations\"][1][\"content\"])[\"answer\"])\n",
    "print(f\"\\nA/Q consistency: {'‚úÖ All match' if mismatches == 0 else f'‚ùå {mismatches} mismatches'}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Unload teacher\n",
    "del teacher_model, teacher_tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"GPU freed: {torch.cuda.mem_get_info()[0]/1024**3:.1f} GiB available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042529aa",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 2: Fine-Tune A-Agent\n",
    "\n",
    "A-Agent solves MCQs ‚Äî critical for elimination round and defense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "774575aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template, standardize_sharegpt, train_on_responses_only\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "print(\"Libraries loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3770b9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 150 A-Agent training examples\n",
      "Dataset: Dataset({\n",
      "    features: ['conversations'],\n",
      "    num_rows: 150\n",
      "})\n",
      "Sample:\n",
      "{\n",
      "  \"conversations\": [\n",
      "    {\n",
      "      \"content\": \"Question: Statement I: All cats are animals.\\nStatement II: Some animals are mammals.\\nConclusion I: Some cats are mammals.\\nConclusion II: All animals are mammals.\\nChoices: A) Only conclusion I follows B) Only conclusion II follows C) Both I and II follow D) Neither I nor II follows\\n\\nSolve step by step and output JSON: {\\\"answer\\\": \\\"<letter>\\\", \\\"reasoning\\\": \\\"<brief>\\\"}\",\n",
      "      \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "      \"content\": \"{\\\"answer\\\": \\\"D\\\",\n"
     ]
    }
   ],
   "source": [
    "# ========== LOAD A-AGENT TRAINING DATA ==========\n",
    "with open(\"training_data/a_agent_train.json\", 'r') as f:\n",
    "    a_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(a_data)} A-Agent training examples\")\n",
    "a_dataset = Dataset.from_list(a_data)\n",
    "print(f\"Dataset: {a_dataset}\")\n",
    "print(f\"Sample:\\n{json.dumps(a_dataset[0], indent=2)[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a31d535b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.10.9: Fast Qwen3 patching. Transformers: 4.56.2. vLLM: 0.11.1rc3.dev39+gf417746ad.rocm700.\n",
      "   \\\\   /|    . Num GPUs = 1. Max memory: 255.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0a0+git1c57644. ROCm Toolkit: 7.0.51831-a3e329ad8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "[2026-02-15 08:43:40] INFO modeling.py:987: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c983d181c21045ef803cee1a1a58f3c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen3-4B loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.10.9 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapters added (r=64, alpha=128).\n"
     ]
    }
   ],
   "source": [
    "# ========== LOAD QWEN3-4B FOR A-AGENT FINE-TUNING ==========\n",
    "max_seq_length = 1024  # MCQ data is short ‚Äî 1024 is plenty, saves VRAM & time\n",
    "dtype = torch.bfloat16  # ROCm compatible\n",
    "load_in_4bit = False\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"./hf_models/Qwen3-4B\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "print(\"Qwen3-4B loaded.\")\n",
    "\n",
    "# Add LoRA adapters ‚Äî all projection layers, rank 64\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "print(\"LoRA adapters added (r=64, alpha=128).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42a2457c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 150. Reducing num_proc to 150 for dataset of size 150.\n",
      "[2026-02-15 08:44:07] WARNING arrow_dataset.py:3114: num_proc must be <= 150. Reducing num_proc to 150 for dataset of size 150.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "246def2b539c490c9e4ddad783b0b739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Standardizing formats (num_proc=150):   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff042aea7a0d4d2f8557fbcdd15051e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0706c02fc64a2688d563090b250195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 150 A-Agent examples\n",
      "Sample: <|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Question: Statement I: All cats are animals.\n",
      "Statement II: Some animals are mammals.\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# ========== PREPARE A-AGENT DATASET ==========\n",
    "tokenizer = get_chat_template(tokenizer, chat_template=\"qwen-2.5\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    for convo in examples[\"conversations\"]:\n",
    "        if isinstance(convo, list) and all(isinstance(m, dict) for m in convo):\n",
    "            texts.append(tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False))\n",
    "    return {\"text\": texts}\n",
    "\n",
    "a_dataset = standardize_sharegpt(a_dataset)\n",
    "a_dataset = a_dataset.map(formatting_prompts_func, batched=True, remove_columns=a_dataset.column_names)\n",
    "a_dataset = a_dataset.filter(lambda x: len(x[\"text\"].strip()) > 0)\n",
    "print(f\"Prepared {len(a_dataset)} A-Agent examples\")\n",
    "if len(a_dataset) > 0:\n",
    "    print(f\"Sample: {a_dataset['text'][0][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f30a55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849616880f6c4dc18035eaa9869dfc50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=64):   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10190cf4e9bf4b84bf48d4030b3c545a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=64):   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting A-Agent training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 150 | Num Epochs = 2 | Total steps = 6\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (32 x 2 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 132,120,576 of 4,154,588,672 (3.18% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n",
      "A-Agent done! Loss: 0.6923\n"
     ]
    }
   ],
   "source": [
    "# ========== TRAIN A-AGENT ==========\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=a_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    "    packing=False,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=32,\n",
    "        gradient_accumulation_steps=2,\n",
    "        warmup_steps=5,\n",
    "        num_train_epochs=2,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=5,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=3407,\n",
    "        output_dir=\"a_agent_training_output\",\n",
    "        report_to=\"none\",\n",
    "        bf16=True,\n",
    "        dataloader_pin_memory=False,\n",
    "        remove_unused_columns=True,\n",
    "        gradient_checkpointing=True,\n",
    "        dataloader_num_workers=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"<|im_start|>user\\n\",\n",
    "    response_part=\"<|im_start|>assistant\\n\",\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_training(model)\n",
    "print(\"Starting A-Agent training...\")\n",
    "trainer_stats = trainer.train()\n",
    "print(f\"A-Agent done! Loss: {trainer_stats.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c87bac8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving A-Agent to ./hf_models/a_agent_finetuned...\n",
      "Detected local model directory: /workspace/AAIPL/hf_models/Qwen3-4B\n",
      "Warning: Found cache directory /root/.cache/huggingface/hub, but lack R/W/X permissions. Cannot use cache.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 34568.44it/s]\n",
      "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:14<00:00,  4.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/workspace/AAIPL/hf_models/a_agent_finetuned`\n",
      "A-Agent saved.\n",
      "GPU memory freed.\n"
     ]
    }
   ],
   "source": [
    "# ========== SAVE A-AGENT MODEL ==========\n",
    "import gc\n",
    "\n",
    "a_merged_path = \"./hf_models/a_agent_finetuned\"\n",
    "print(f\"Saving A-Agent to {a_merged_path}...\")\n",
    "model.save_pretrained_merged(a_merged_path, tokenizer, save_method=\"merged_16bit\")\n",
    "print(f\"A-Agent saved.\")\n",
    "\n",
    "# Free GPU memory\n",
    "del model, trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"GPU memory freed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a526e66e",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 3: Fine-Tune Q-Agent\n",
    "\n",
    "Q-Agent generates hard MCQs ‚Äî scores when opponent's A-Agent fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9910eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 150 Q-Agent training examples\n",
      "Sample:\n",
      "{\n",
      "  \"conversations\": [\n",
      "    {\n",
      "      \"content\": \"Generate a difficult MCQ on topic: Logical Reasoning/Syllogisms. Output ONLY valid JSON.\",\n",
      "      \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "      \"content\": \"{\\\"topic\\\": \\\"Logical Reasoning/Syllogisms\\\", \\\"question\\\": \\\"Statement I: All cats are animals.\\\\nStatement II: Some animals are mammals.\\\\nConclusion I: Some cats are mammals.\\\\nConclusion II: All animals are mammals.\\\", \\\"choices\\\": [\\\"A) Only conclusion I follows\\\", \\\"B) Only conclusion II follows\\\", \\\"C)\n"
     ]
    }
   ],
   "source": [
    "# ========== LOAD Q-AGENT TRAINING DATA ==========\n",
    "with open(\"training_data/q_agent_train.json\", 'r') as f:\n",
    "    q_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(q_data)} Q-Agent training examples\")\n",
    "q_dataset = Dataset.from_list(q_data)\n",
    "print(f\"Sample:\\n{json.dumps(q_dataset[0], indent=2)[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b3bbca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.10.9: Fast Qwen3 patching. Transformers: 4.56.2. vLLM: 0.11.1rc3.dev39+gf417746ad.rocm700.\n",
      "   \\\\   /|    . Num GPUs = 1. Max memory: 255.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0a0+git1c57644. ROCm Toolkit: 7.0.51831-a3e329ad8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-15 08:50:03] INFO modeling.py:987: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22bffa4dc5c3403c892482ec471b335f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fresh Qwen3-4B loaded for Q-Agent (r=64).\n"
     ]
    }
   ],
   "source": [
    "# ========== LOAD FRESH QWEN3-4B FOR Q-AGENT ==========\n",
    "max_seq_length = 1024  # MCQ data is short ‚Äî 1024 saves time\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"./hf_models/Qwen3-4B\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=torch.bfloat16,\n",
    "    load_in_4bit=False,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "print(\"Fresh Qwen3-4B loaded for Q-Agent (r=64).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c92d1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 150. Reducing num_proc to 150 for dataset of size 150.\n",
      "[2026-02-15 08:51:08] WARNING arrow_dataset.py:3114: num_proc must be <= 150. Reducing num_proc to 150 for dataset of size 150.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b3b232b61044b14ad02bdeec9b302f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Standardizing formats (num_proc=150):   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0e0b7077234d31a5e50797758a520b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807b9ed8ea004f92b6f81474f31bcbe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 150 Q-Agent training examples\n"
     ]
    }
   ],
   "source": [
    "# ========== PREPARE Q-AGENT DATASET ==========\n",
    "tokenizer = get_chat_template(tokenizer, chat_template=\"qwen-2.5\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "q_dataset = standardize_sharegpt(q_dataset)\n",
    "q_dataset = q_dataset.map(formatting_prompts_func, batched=True, remove_columns=q_dataset.column_names)\n",
    "q_dataset = q_dataset.filter(lambda x: len(x[\"text\"].strip()) > 0)\n",
    "\n",
    "print(f\"Prepared {len(q_dataset)} Q-Agent training examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a834f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a6cc7ebd6614864a1863719a9e97fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=64):   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e701079d974045ba02b9c053145c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=64):   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Q-Agent training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 150 | Num Epochs = 2 | Total steps = 6\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (32 x 2 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 132,120,576 of 4,154,588,672 (3.18% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:06, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.657700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Agent done! Loss: 0.6028\n"
     ]
    }
   ],
   "source": [
    "# ========== TRAIN Q-AGENT ==========\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=q_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    "    packing=False,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=32,\n",
    "        gradient_accumulation_steps=2,\n",
    "        warmup_steps=5,\n",
    "        num_train_epochs=2,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=5,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=3407,\n",
    "        output_dir=\"q_agent_training_output\",\n",
    "        report_to=\"none\",\n",
    "        bf16=True,\n",
    "        dataloader_pin_memory=False,\n",
    "        remove_unused_columns=True,\n",
    "        gradient_checkpointing=True,\n",
    "        dataloader_num_workers=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"<|im_start|>user\\n\",\n",
    "    response_part=\"<|im_start|>assistant\\n\",\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_training(model)\n",
    "print(\"Starting Q-Agent training...\")\n",
    "trainer_stats = trainer.train()\n",
    "print(f\"Q-Agent done! Loss: {trainer_stats.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3778a247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Q-Agent to ./hf_models/q_agent_finetuned...\n",
      "Detected local model directory: /workspace/AAIPL/hf_models/Qwen3-4B\n",
      "Warning: Found cache directory /root/.cache/huggingface/hub, but lack R/W/X permissions. Cannot use cache.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 14429.94it/s]\n",
      "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/workspace/AAIPL/hf_models/q_agent_finetuned`\n",
      "Q-Agent saved.\n",
      "Both fine-tuned models saved.\n"
     ]
    }
   ],
   "source": [
    "# ========== SAVE Q-AGENT MODEL ==========\n",
    "q_merged_path = \"./hf_models/q_agent_finetuned\"\n",
    "print(f\"Saving Q-Agent to {q_merged_path}...\")\n",
    "model.save_pretrained_merged(q_merged_path, tokenizer, save_method=\"merged_16bit\")\n",
    "print(f\"Q-Agent saved.\")\n",
    "\n",
    "del model, trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Both fine-tuned models saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b200f38",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 4: Update Model Paths and Test\n",
    "\n",
    "Point `question_model.py` and `answer_model.py` to the fine-tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fcf6acc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ agents/question_model.py -> q_agent_finetuned\n",
      "‚úÖ agents/answer_model.py -> a_agent_finetuned\n",
      "  agents/question_model.py: path=True thinking=True strip=True\n",
      "  agents/answer_model.py: path=True thinking=True strip=True\n"
     ]
    }
   ],
   "source": [
    "# ========== UPDATE MODEL PATHS + FIX QWEN3 <think> TAGS + ROBUST JSON ==========\n",
    "import re as _re\n",
    "\n",
    "for fname, new_path in [(\"agents/question_model.py\", \"q_agent_finetuned\"),\n",
    "                         (\"agents/answer_model.py\", \"a_agent_finetuned\")]:\n",
    "    code = open(fname, \"r\").read()\n",
    "    \n",
    "    # 1. Fix MODEL_PATH (handle both old and already-updated paths)\n",
    "    code = _re.sub(\n",
    "        r'MODEL_PATH\\s*=\\s*str\\(Path\\(__file__\\)\\.parent\\.parent\\s*/\\s*\"hf_models\"\\s*/\\s*\"[^\"]+\"\\)',\n",
    "        f'MODEL_PATH = str(Path(__file__).parent.parent / \"hf_models\" / \"{new_path}\")',\n",
    "        code\n",
    "    )\n",
    "    \n",
    "    # 2. Add enable_thinking=False if missing\n",
    "    if \"enable_thinking=False\" not in code:\n",
    "        code = code.replace(\n",
    "            \"add_generation_prompt=True,\\n            )\",\n",
    "            \"add_generation_prompt=True,\\n                enable_thinking=False,\\n            )\"\n",
    "        )\n",
    "    \n",
    "    # 3. Add <think> tag stripping if missing\n",
    "    if \"re.sub(r'<think>\" not in code:\n",
    "        if \"import re\\n\" not in code:\n",
    "            code = code.replace(\"import time\\n\", \"import re\\nimport time\\n\", 1)\n",
    "        code = code.replace(\n",
    "            '.strip(\"\\\\n\")\\n            batch_outs.append(content)',\n",
    "            '.strip(\"\\\\n\")\\n            # Strip Qwen3 <think> tags if present\\n            content = re.sub(r\\'<think>.*?</think>\\', \\'\\', content, flags=re.DOTALL).strip()\\n            batch_outs.append(content)'\n",
    "        )\n",
    "    \n",
    "    open(fname, \"w\").write(code)\n",
    "    print(f\"‚úÖ {fname} -> {new_path}\")\n",
    "\n",
    "# Verify the changes\n",
    "for fname in [\"agents/question_model.py\", \"agents/answer_model.py\"]:\n",
    "    code = open(fname).read()\n",
    "    has_path = \"finetuned\" in code\n",
    "    has_think = \"enable_thinking=False\" in code\n",
    "    has_strip = \"re.sub\" in code\n",
    "    print(f\"  {fname}: path={has_path} thinking={has_think} strip={has_strip}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fefc934f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:410: UserWarning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (Triggered internally at /app/pytorch/c10/core/AllocatorConfig.cpp:28.)\n",
      "  torch._C._cuda_init()\n",
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "#### Unsloth: `hf_xet==1.1.10` and `ipykernel>6.30.1` breaks progress bars. Disabling for now in XET.\n",
      "#### Unsloth: To re-enable progress bars, please downgrade to `ipykernel==6.30.1` or wait for a fix to\n",
      "https://github.com/huggingface/xet-core/issues/526\n",
      "INFO 02-15 09:17:11 [__init__.py:225] Automatically detected platform rocm.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "/usr/local/lib/python3.12/dist-packages/unsloth_zoo/gradient_checkpointing.py:348: UserWarning: expandable_segments not supported on this platform (Triggered internally at /app/pytorch/c10/hip/HIPAllocatorConfig.h:39.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"{DEVICE_TYPE_TORCH}:{i}\") for i in range(n_gpus)])\n",
      "==((====))==  Unsloth 2025.10.9: Fast Qwen3 patching. Transformers: 4.56.2. vLLM: 0.11.1rc3.dev39+gf417746ad.rocm700.\n",
      "   \\\\   /|    . Num GPUs = 1. Max memory: 255.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0a0+git1c57644. ROCm Toolkit: 7.0.51831-a3e329ad8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "[2026-02-15 09:17:15] INFO modeling.py:987: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.08s/it]\n",
      "STEPS:   0%|                                              | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/unsloth/kernels/utils.py:743: UserWarning: An output with one or more elements was resized since it had shape [1, 5, 2560], which does not match the required output shape [5, 1, 2560]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /app/pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  out = torch_matmul(X, W.t(), out = out)\n",
      "STEPS:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 1/2 [00:27<00:27, 27.77s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "STEPS: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:46<00:00, 23.12s/it]\n",
      "Generated 10 questions!\n",
      "{\"!topic\":\"Logical Reasoning/Syllogisms\",\"!question\":\"Statements:\\nI. Some books are novels.\\nII. All novels are stories.\\nIII. Some stories are poems.\\n\\nConclusion I: Some poems are books.\\nConclusion II: Some stories are novels.\",\"!choices\":[\"A) A) Only Conclusion I follows\",\"B) B) Only Conclusion II follows\",\"C) C) Both I and II follow\",\"D) D) Neither I nor II follows\"],\"!answer\":\"D\",\"!explanation\"} {\"topic\": \"Logical Reasoning/Syllogisms\", \"question\": \"Statements:\\nI. Some books are novels.\\nII. All novels are stories.\\nIII. Some stories are poems.\", \"choices\": [\"A) A) If only conclusion I follows\", \"B) B) If only conclusion II follows\", \"C) C) If conclusions I and II both follow\", \"D) D) If neither conclusion I nor conclusion II follows\"], \"answer\": \"\", \"explanation\": \"\"}{\"!\"}{\"topic\": \"Logical Reasoning/Syllogisms\", \"question\": \"Statements:\\nI. Some books are novels.\\nII. All novels are stories.\\nIII. Some stories are poems.\\n\\nConclusion I: Some poems are books.\\nConclusion II: Some stories are novels.\"}{\"!\"}{\"topic\": \"Logical Reasoning/Syllogisms\", \"question\": \"Statements:\\nI. Some books are novels.\\nII. All novels are stories.\\nIII. Some stories are poems.\\n\\nConclusion I: Some poems are books.\\nConclusion II: Some stories are novels.\"}{\"!\"}{\"topic\": \"Logical Reasoning/Syllogisms\", \"question\": \"Statements:\\nI. Some books are novels.\\nII. All novels are stories.\\nIII. Some stories are poems.\\n\\nConclusion I: Some poems are books.\\nConclusion II: Some stories are novels.\"}{\"!\"}{\"topic\": \"Logical Reasoning/Syllogisms\", \"question\": \"Statements:\\nI. Some books are novels.\\nII. All novels are stories.\\nIII. Some stories are poems.\\n\\nConclusion I: Some poems are books.\\nConclusion II: Some stories are novels.\"}{\"!\"}{\"topic\": \"Logical Reasoning/Syllogisms\", \"question\": \"Statements:\\nI. Some books are novels.\\nII. All novels are stories.\\nIII. Some stories are poems.\\n\\nConclusion I: Some poems are books.\\nConclusion II: Some stories are novels.\"}{\"!\"}{\"topic\": \"Logical Reasoning/Syllogisms\", \"question\": \"Statements:\\nI. Some books are novels.\\nII. All novels are stories.\\nIII. Some stories are poems.\\n\\nConclusion I: Some poems are books.\\nConclusion II: Some stories are novels.\"}{\"!\"}{\"topic\": \"Logical Reasoning/Syllogisms\", \"question\": \"Statements:\\nI. Some books are novels.\\nII. All novels are stories.\\nIII. Some stories are poems.\\n\\nConclusion I: Some poems are books.\\nConclusion II: Some stories are novels.\"}{\"!\"}{\"topic\": \"Logical Reasoning/Syllogisms\", \"question\": \"Statements:\\nI. Some books are novels.\\nII. All novels are stories.\\nIII. Some stories are poems.\\n\\nConclusion I: Some poems are books.\\nConclusion II: Some stories are novels.\"}{\"!\"}{\"topic\": \"Logical Reasoning/Syllogisms\", \"question\": \"Statements:\\nI. Some books are novels.\\nII. All novels are stories.\\nIII. Some stories are poems.\\n\\nConclusion I: Some poems are books.\\nConclusion II: Some stories are novels.\"}{\"!\"}{\"topic\": \"Logical Reasoning/Syllogisms\", \"question\": \"Statements:\\nI. Some books are novels.\\nII. All novels are stories.\\nIII. Some stories are poems.\\n\\nConclusion I: Some poems are books.\\nConclusion II: Some stories are novels.\"}{\"!\"}{\"topic\": \"Logical Reasoning/Syllogisms\", \"question\": \"Statements:\\nI. Some books are novels.\\nII. All novels are stories.\\nIII. Some stories are poems.\\n\\nConclusion I: Some poems are books.\\nConclusion II: Some stories are novels.\"}{\"!\"}{\"topic\": \"Logical Reasoning/Syllogisms\", \"question\": \"Statements:\\nI. Some books are novels.\\nII. All novels are stories.\\nIII. Some stories are poems.\\n\\nConclusion I: Some poems are books.\\nConclusion II: Some stories are novels.\"}{\"!\"}{\"topic\": \"Logical Reasoning/Syllogisms\", \"question\": \"Statements:\\nI. Some books are novels.\\\n",
      "{\"exit_code\": 0} {\"topic\": \"Logical Reasoning/Syllogisms\"}{\"question\": \"Statements:\\nI) Only one person who speaks English can attend this meeting.\\nII) Three people will definitely speak Spanish at the event.\"}\"choices\": [\"A\") \"If statement I implies statement II\",\"B\") \"If statements I and II contradict each other\",\"C\") \"If both statements could possibly be true simultaneously\",\"D\") \"None of these\"]{\"answer\": \"C\"}{\"explanation\": \"Only choice C allows both statements to hold without contradiction. Statement A assumes exclusivity where none is stated, while B claims direct opposition when no logical conflict exists.\"}{\"exit_code\": -1}{\"topic\": \"Logical Reasoning/Syllogisms\"}{\"question\": \"Statements:\\nI) At least two participants from Country X have signed up.\\nII) More than half of all attendees are non-English speakers.\"}\"choices\": [\"A\") \"Both statements are necessarily false if there are five participants\",\"B\") \"Exactly four participants from Country Y attended\",\"C\") \"It's impossible for more than six people to have signed up\",\"D\") \"The number of participants could vary widely\"]}{\"answer\": \"A\"}{\"explanation\": \"Choice A correctly identifies that if there were five participants, it would violate either statement I or II due to insufficient information provided about language proficiency distribution among attendees.\"}{\"exit_code\": -2}{\"topic\": \"Logical Reasoning/Syllogisms\"}{\"question\": \"Statements:\\nI) Every student who passed the exam has studied diligently.\\nII) Some students did not study hard enough.\"}\"choices\": [\"A\") \"At most eight students failed the exam\",\"B\") \"More than ten students took the exam\",\"C\") \"Some students passed despite not studying hard\",\"\\nD) \"){\"answer\": \"C\"}{\"explanation\": \"Option C directly contradicts statement I because it suggests passing without diligent study, making it logically inconsistent with the given premises.\"}{\"exit_code\": -3}{\"topic\": \"Logical Reasoning/Syllogisms\"}{\"question\": \"Statements:\\nI) Exactly seven people visited the museum yesterday.\\nII) None of those visitors wore sunglasses.\",\"choices\": [\"A\") \"Five people arrived before noon\",\"B\") \"Three individuals left after sunset\",\"C\") \"Two tourists came from outside the city\",\"D) \"){\"answer\": \"C\"}{\"explanation\": \"Choice C introduces new information unrelated to the original statements, allowing for consistent interpretation across different scenarios without violating either premise.\"}{\"exit_code\": -4}{\"topic\": \"Logical Reasoning/Syllogisms\"}{\"question\": \"Statements:\\nI) Only registered voters may enter the polling station.\\nII) John was seen leaving the venue shortly after midnight.\"}\"choices\": [\"A\") \"John must be an unregistered voter\",\"B\") \"John entered through the back entrance\",\"C\") \"There might be additional security measures in place\",\"D) \"){\"answer\": \"A\"}{\"explanation\": \"Based solely on the given information, we cannot definitively conclude anything about John's registration status beyond what is explicitly stated in the premises.\"}{\"exit_code\": -5}{\"topic\": \"Logical Reasoning/Syllogisms\"}{\"question\": \"Statements:\\nI) If it rains tomorrow, then the picnic will be canceled.\\nII) It is currently sunny today.\"}\"choices\": [\"A\") \"Tomorrow will also be cloudy\",\"B\") \"The picnic is guaranteed to happen \",\"C\") \"There is a chance of thunderstorms tonight\",\"D) \"){\"answer\": \"B\"}{\"explanation\": \"Since it is currently sunny today, there is no immediate reason to believe it will rain tomorrow based solely on current conditions.\"}{\"exit_code\": -6}{\"topic\": \"Logical Reasoning/Syllogisms\"}{\"question\": \"Statements:\\nI) Sarah received her promotion last week.\\nII) Her team members noticed she started working overtime earlier this month.\"}\"choices\": [\"A\") \"Sarah now works full-time hours every day\",\"B\") \"She began taking unpaid leave frequently\",\"C\") \"Her manager expressed concern about her workload\",\"D) \"){\"answer\": \"A\"}{\"explanation\": \"Given that Sarah received her promotion last week and her team observed increased work hours during the preceding month, it strongly suggests she transitioned into full-time employment circumstances.\"}{\"exit_code\": -7}{\"topic\": \"Logical Reasoning/Syllogisms\"}{\"question\": \"Statements:\\nI) Michael won first prize in the spelling bee competition.\\nII) He spent over $50 on pencils throughout the contest period.\"}\"choices\": [\"A\") \"Michael used all his spare change to buy supplies\",\"B\") \"He earned extra money by helping others practice\",\"C\") \"His victory was largely due to luck rather than skill\",\"D) \"){\"answer\n",
      "{\"!topic\":\"Series and Patterns/Mixed Series\",\"!question\":\"(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z)\",\"!choices\":[\"A) A) QRS\",\"B) B) TUV\",\"C) C) VWX\",\"D) D) XYZ\"],\"!answer\":\"C\",\"!explanation\":{\"step1\":\"Step 1: Identify the pattern.\",\"step2\":\"Step 2: Determine how each element progresses.\",\"step3\":\"Step 3: Confirm that choice C fits this progression.\"}} {\"!\"topic\"} {\"!question\"} {\"!\"choices\"} {\"!\"answer\"} {\"!\"explanation\"} {\"!description\"} {\"!example\"} {\"!difficulty\"} {\"!category\"} {\"!tagged\"} {\"!related_topics\"} {\"!additional_info\"} {\"!solution_steps\"} {\"!key_concepts\"} {\"!important_points\"} {\"!common_mistakes\"} {\"!note_to_student\"} {\"!study_tip\"} {\"!practice_questions\"} {\"!quizlet_terms\"} {\"!flashcards\"} {\"!bookmarks\"} {\"!further_reading\"} {\"!online_resources\"} {\"!video_tutorials\"} {\"!audio_material\"} {\"!interactive_activities\"} {\"!games_and_puzzles\"} {\"!assessment_tools\"} {\"!certifications\"} {\"!degrees\"} {\"!courses\"} {\"!degree_courses\"} {\"!professional_certification\"} {\"!workshops\"} {\"!seminars\"} {\"!conferences\"} {\"!symposiums\"} {\"!meetings\"} {\"!webinars\"} {\"!videos\"} {\"!podcasts\"} {\"!articles\"} {\"!books\"} {\"!magazines\"} {\"!journals\"} {\"!websites\"} {\"!blogs\"} {\"!forums\"} {\"!social_media_groups\"} {\"!communities\"} {\"!networks\"} {\"!clubs\"} {\"!associations\"} {\"!organizations\"} {\"!institutions\"} {\"!companies\"} {\"!startups\"} {\"!entrepreneurs\"} {\"!investors\"} {\"!partnerships\"} {\"!collaborations\"} {\"!alliances\"} {\"!strategic_alliances\"} {\"!joint Ventures\"} {\"!mergers\"} {\"!acquisitions\"} {\"!takeovers\"} {\"!consolidations\"} {\"!spin-offs\"} {\"!divestitures\"} {\"!restructurings\"} {\"!reflections\"} {\"!reviews\"} {\"!summaries\"} {\"!overviews\"} {\"!introductions\"} {\"!guides\"} {\"!manuals\"} {\"!handbooks\"} {\"!cheat_sheets\"} {\"!quick_reference_cards\"} {\"!toolkits\"} {\"!bundles\"} {\"!packages\"} {\"!sets\"} {\"!collections\"} {\"!groups\"} {\"!clusters\"} {\"!categories\"} {\"!types\"} {\"!kinds\"} {\"!forms\"} {\"!versions\"} {\"!editions\"} {\"!copies\"} {\"!duplicates\"} {\"!replicas\"} {\"!exact_copies\"} {\"!originals\"} {\"!rare_editions\"} {\"!limited_editions\"} {\"!exclusive_versions\"} {\"!collector_items\"} {\"!collectibles\"} {\"!memorabilia\"} {\"!artworks\"} {\"!posters\"} {\"!prints\"} {\"!stickers\"} {\"!labels\"} {\"!tapes\"} {\"!cassettes\"} {\"!vinyl_records\"} {\"!cds\"} {\"!dvd-rs\"} {\"!blu-rays\"} {\"!mp3_files\"} {\"!wav_files\"} {\"!ogg_files\"} {\"!aac_files\"} {\"!flac_files\"} {\"!alac_files\"} {\"!mka_files\"} {\"!wma_files\"} {\"!mp4_videos\"} {\"!mpeg_videos\"} {\"!avi_videos\"} {\"!mkv_videos\"} {\"!mov_videos\"} {\"!wmv_videos\"} {\"!rmvb_videos\"} {\"!qt_videos\"} {\"!ts_videos\"} {\"!dvds\"} {\"!cds\"} {\"!lasers\"} {\"!platters\"} {\"!discs\"} {\"!stamps\"} {\"!coins\"} {\"!bottles\"} {\"!boxes\"} {\"!bags\"} {\"!cases\"} {\"!containers\"} {\"!packaging\"} {\"!wrappers\"} {\"!covers\"} {\"!labels\"} {\"!tags\"} {\"!indexes\"} {\"!directories\"} {\"!folders\"} {\"!sub-folders\"} {\"!trees\"} {\"!graphs\"} {\"!hierarchies\"} {\"!diagrams\"} {\"!charts\"} {\"!tables\"} {\"!lists\"} {\"!queues\"} {\"#stack\"} {\"#queue\"} {\"#priority_queue\"} {\"#heap\"} {\"#tree\"} {\"#graph\"} {\"#digraph\"} {\"#undirected_graph\"} {\"#directed_graph\"} {\"#weighted_graph\"} {\"#unweighted_graph\"} {\"#adjacency_list\"} {\"#adjacency_matrix\"} {\"#incidence_matrix\"} {\"#transitive_closure\"} {\"#topological_sort\"} {\"#minimum_spanning\n",
      "{\"!topic\":\"Syllogisms\",\"!question\":\"Statements:\\nI) All cars have engines.\\nII) Some engineers drive cars.\\nIII) Only one person can repair a car at a time.\\n\\nConclusions:\\nI) At least two people work together repairing vehicles.\\nII) Engineers may own cars.\",\"!choices\":[\"A)\",\"B\")],\"!\"answer\":\"B\"}{\"topic\":\"Syllogisms\",\"question\":\"Statements:\\nI) All cats are mammals.\\nII) Some dogs are pets.\\nIII) No reptile is a mammal.\\n\\nConclusions:\\nI) Some pets are reptiles.\\nII) Cats might not be pets.\",\"choices\":[\"A)\",\n",
      "\"B)\", \n",
      "\"C)\", \n",
      "\"D)\"],\"answer\":\"B\",\"explanation\":{\"step1\": {\"action\": \"analyze statements\"}}\n",
      "{\"topic\":\"Blood relations\",\"question\":\"Ravi says,\\\"Poonam is the aunt of my cousin Mohit.\\\"How is Poonam related to Ravi?\"},{\"choices\":[\"A)\",\"A) Step-sister\",\"B\"),\"B) Cousin\",\"C\")\",\"C) Sister-in-law\",\"D\")\",\"D) Niece\"],\"answer\":\"B\"}{\"topic\":\"Blood relations\",\"question\":\"Shyama says,\\\"Gopal is the nephew of my mother's brother.\"},{\"choices\":[\"A)\"\"A) Son\",\"B\")\",\"B) Cousin\",\"C\")\",\"C) Brother\",\"D\")\",\"D) Nephew\"]},{\"answer\":\"B\"},{\"explanation\":\"Step-by-step Explanation:\\n\\n1. Shyama mentions Gopal being the nephew of her mother's brother.\\n2. First identify relationships from outermost layer: Shyama's mother's brother = maternal uncle.\\n3. Gopal being the nephew of this person means he is also Shyama's cousin.\\n4. However, since Gopal is specifically described as a nephew,\\n5. This indicates that Gopal has at least one parent who shares blood relation with Shyama through marriage.\\n6. In family terms, if two people share a direct ancestor-descendant relationship via either biological parents or married siblings, they become cousins.\\n7. But because Gopal was explicitly called \\\"nephew\\\",\\n8. It implies there might be additional familial connections beyond just siblinghood.\"){\"topic\":\"Blood relations\",\"question\":\"Deepak says,\\\"Radha is the daughter of my paternal grandmother's only child\\\".\",\"choices\":[{\"A)\"=\"A) Sister\",\"B)\"=\"B) Cousin\",\"C)\"=\"C) Daughter\",\"D)\"=\"D) Aunt\"}],\"answer\":\"B\",\"explanation\":\"Step-by-step Explanation:\\n\\n1. Deepak states Radha is the daughter of his paternal grandmother's only child.\\n2. Paternal grandmother means she is the mother of Deepak's father.\\n3. If she has only one child, that would be Deepak himself.\\n4. So Radha becomes the daughter of Deepak's own parent.\\n5. This makes Radha both Deepak's sister and his niece simultaneously.\\n6. Among the given choices, neither A nor D directly match these dual roles.\\n7. Choice C incorrectly assumes Radha is solely Deepak's sister while ignoring their shared ancestry.\\n8. Hence, the most accurate description based on the information provided is choice B.\"{\"topic\":\"Blood relations\",\"question\":\"Priya says,\\\"Ajay is the brother of my paternal aunt's son\\\".\",\"choices\":[[\"A) A) Cousin\",\"B) B) Nephew\",\"C) C) Brother\",\"D) D) Uncle\"], {\"topic\":\"Blood relations\",\"question\":\"Anand says,\\\"Kiran is the grandson of my mother's only child\\\".\",\"choices\":[[\"A) A) Brother\",\"B) B) Cousin\",\"C) C) Son\",\"D) D) Niece\"]}{\"topic\":\"Blood relations\",\"question\":\"Meera tells you that Rajesh is the son of her father's younger brother.\"},{\"choices\":[\"A)\", \"A) Brother\",\"B)\", \"B) Cousin\",\"C)\", \"C) Nephew\",\"D)\", \"D) Uncle\"], \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"Blood relations\", \"topic\": \"\n",
      "step-by-step{\"topic\":\"Blood Relations\",\"question\":\"Punit states,\" She is the aunt of my cousin.\" What relation does she hold?\"\"} {\"choices\":[\"A)\",\"B\")\",\"C\")\",\"D\")}{\"answer\":\"C\"}{\"explanation\":\"Step 1: Punit mentions \\\"my cousin\\\"‚Äîthis indicates their sibling relationship.\\nStep 2: Step 3:\"{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\": \"Blood relations\"}{\"topic\": \"Blood relations\",\"topic\": \"Blood relations\",\"topic\n",
      "{\"ready\"}{\"topic\":\"Syllogisms\",\"question\":\"Statements:\\nI. Only one person reads Hindi.\\nII. At least two people speak English.\",\"choices\":[\"A)\",\"If only statement I holds\"],\"B\")\",\"if both statements hold\"},{\"c\")\",\"If none hold\"),\"d\")\",\"otherwise\")}{\"answer\":\"b\"}{\"explanation\":\"\"}{\"step-by-step\":true}{\"topic\":\"Logical Reasoning/Syllogisms\"}{\"topic\":\"Logics\"}{\"topic\":\"Analytical Reasoning\"}{\"topic\":\"Reasoning\"}{\"topic\":\"Logic\"}{\"topic\":\"Data Interpretation\"}{\"topic\":\"Problem Solving\"}{\"topic\":\"Critical Thinking\"}{\"topic\":\"Verbal Ability\"}{\"topic\":\"Reading Comprehension\"}{\"topic\":\"Grammar\"}{\"topic\":\"Sentence Correction\"}{\"topic\":\"Vocabulary\"}{\"topic\":\"Idioms\"}{\"topic\":\"Synonyms\"}{\"topic\":\"Antonyms\"}{\"topic\":\"Word Usage\"}{\"topic\":\"Punctuation\"}{\"topic\":\"Comma Splices\"}{\"topic\":\"Fragments\"}{\"topic\":\"Run-ons\"}{\"topic\":\"Parallel Structure\"}{\"topic\":\"Tone\"}{\"topic\":\"Persuasion\"}{\"topic\":\"Argumentation\"}{\"topic\":\"Inference\"}{\"topic\":\"Assumption\"}{\"topic\":\"Weakening\"}{\"topic\":\"Strengthening\"}{\"topic\":\"General Knowledge\"}{\"topic\":\"Current Affairs\"}{\"topic\":\"History\"}{\"topic\":\"Geography\"}{\"topic\":\"Politics\"}{\"topic\":\"Economy\"}{\"topic\":\"Science\"}{\"topic\":\"Technology\"}{\"topic\":\"Environment\"}{\"topic\":\"Culture\"}{\"topic\":\"Sports\"}{\"topic\":\"Books\"}{\"topic\":\"Films\"}{\"topic\":\"Music\"}{\"topic\":\"Art\"}{\"topic\":\"Theater\"}{\"topic\":\"Literature\"}{\"topic\":\"Poetry\"}{\"topic\":\"Prose\"}{\"topic\":\"Essay Writing\"}{\"topic\":\"Speech Writing\"}{\"topic\":\"Letter Writing\"}{\"topic\":\"Email Writing\"}{\"topic\":\"Application Letters\"}{\"topic\":\"Cover Letters\"}{\"topic\":\"Resumes\"}{\"topic\":\"Interview Preparation\"}{\"topic\":\"Common Interview Questions\"}{\"topic\":\"Behavioral Interviews\"}{\"topic\":\"Technical Interviews\"}{\"topic\":\"Phone Screenings\"}{\"topic\":\"Hiring Managers\"}{\"topic\":\"Career Development\"}{\"topic\":\"Job Search\"}{\"topic\":\"Networking\"}{\"topic\":\"LinkedIn\"}{\"topic\":\"Resume Optimization\"}{\"topic\":\"Portfolio Building\"}{\"topic\":\"Freelancing\"}{\"topic\":\"Remote Work\"}{\"topic\":\"Work-Life Balance\"}{\"topic\":\"Time Management\"}{\"topic\":\"Productivity Tools\"}{\"topic\":\"Digital Marketing\"}{\"topic\":\"SEO\"}{\"topic\":\"Content Creation\"}{\"topic\":\"Social Media Marketing\"}{\"topic\":\"Email Marketing\"}{\"topic\":\"Pay-Per-Click\"}{\"topic\":\"Affiliate Marketing\"}{\"topic\":\"Facebook Ads\"}{\"topic\":\"Google AdWords\"}{\"topic\":\"Instagram Ads\"}{\"topic\":\"Twitter Ads\"}{\"topic\":\"YouTube Advertising\"}{\"topic\":\"Branding\"}{\"topic\":\"Logo Design\"}{\"topic\":\"Brand Identity\"}{\"topic\":\"Marketing Strategy\"}{\"topic\":\"Market Research\"}{\"topic\":\"Customer Segmentation\"}{\"topic\":\"Target Audience\"}{\"topic\":\"Positioning\"}{\"topic\":\"Differentiation\"}{\"topic\":\"Value Proposition\"}{\"topic\":\"Competitive Advantage\"}{\"topic\":\"SWOT Analysis\"}{\"topic\":\"PESTEL Analysis\"}{\"topic\":\"Porter's Five Forces\"}{\"topic\":\"Cost-Benefit Analysis\"}{\"topic\":\"ROI Calculation\"}{\"topic\":\"Break-even Point\"}{\"topic\":\"Profit Margin\"}{\"topic\":\"Gross Profit\"}{\"topic\":\"Net Income\"}{\"topic\":\"Balance Sheet\"}{\"topic\":\"Income Statement\"}{\"topic\":\"Cash Flow Statement\"}{\"topic\":\"Financial Ratios\"}{\"topic\":\"Liquidity Ratios\"}{\"topic\":\"Efficiency Ratios\"}{\"topic\":\"Debt Ratios\"}{\"topic\":\"Coverage Ratios\"}{\"topic\":\"Inventory Turnover\"}{\"topic\":\"Accounts Receivable Turnover\"}{\"topic\":\"Asset Utilization Ratio\"}{\"topic\":\"Capital Gearing Ratio\"}{\"topic\":\"Return on Equity\"}{\"topic\":\"Dividend Yield\"}{\"topic\":\"Price-to-Earnings Ratio\"}{\"topic\":\"Enterprise Value Multiples\"}{\"topic\":\"Discounted Cash Flow\"}{\"topic\":\"NPV\"}{\"topic\":\"IRR\"}{\"topic\":\"Payback Period\"}{\"topic\":\"Internal Rate of Return\"}{\"topic\":\"Simple Interest\"}{\"topic\":\"Compound Interest\"}{\"topic\":\"Annuity\"}{\"topic\":\"Perpetuity\"}{\"topic\":\"Future Value\"}{\"topic\":\"Present Value\"}{\"topic\":\"Amortization Schedule\"}{\"topic\":\"Loan Repayment\"}{\"topic\":\"Credit Score\"}{\"topic\":\"Equity Investment\"}{\"topic\":\"Debt Financing\"}{\"topic\":\"Mergers & Acquisitions\"}{\"topic\":\"Stock Market\"}{\"topic\":\"Bond Markets\"}{\"topic\":\"ETFs\"}{\"topic\":\"Mutual Funds\"}{\"topic\":\"Index Fund\"}{\"topic\":\"Fundamental Analysis\"}{\"topic\":\"Technical Analysis\"}{\"topic\":\"Chart Patterns\"}{\"topic\":\"Support\n",
      "1{\"topic\":\"Blood Relation Logic\",\"question\":\"Ravi says,\\u201d\\u2018Ram is the brother of my father's youngest child.\\u2019 Who is Ram to Ravi?\",\"choices\":[\"A) A) Brother\",\"B) B) Cousin\",\"C) C) Nephew\",\"D) D) Stepbrother\"],\"answer\":\"A\",\"explanation\":\"\"}\n",
      "{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"\", \"choices\": [], \"answer\": \"A\", \"explanation\": \"\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"\", \"choices\": [], \"answer\": \"A\", \"explanation\": \"\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"\", \"choices\": [], \"answer\": \"A\", \"explanation\": \"\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"\", \"choices\": [], \"answer\": \"A\", \"explanation\": \"\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"\", \"choices\": [], \"answer\": \"A\", \"explanation\": \"\"}{\"topic\": \"Series andPatterns/Mixed Series (Alphanumeric)\", \"question\": \"\", \"choices\": [], \"answer\": \"A\", \"explanation\": \"\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"\", \"choices\": [], \"answer\": \"A\", \"explanation\": \"\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"\", \"choices\": [], \"answer\": \"A\", \"explanation\": \"\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"\", \"choices\": [], \"answer\": \"A\", \"explanation\": \"\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"\", \"choices\": [], \"answer\": \"A\", \"explanation\": \"\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"\", \"choices\": [], \"answer\": \"A\", \"explanation\": \"\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"\", \"choices\": [], \"answer\": \"A\", \"explanation\": \"\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"\", \"choices\": [], \"answer\": \"A\", \"explanation\": \"\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"\", \"choices\": [], \"answer\": \"A\", \"explanation\": \"\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"\", \"choices\": [], \"answer\": \"A\", \"explanation\": \"\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"\", \"choices\": [], \"answer\": \"A\", \"explanation\": \"\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"\", \"choices\": [], \"answer\": \"A\", \"explanation\": \"\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"\", \"choices\": [], \"answer\": \"A\", \"explanation\": \"\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"\", \"choices\": [], \"answer\": \"A\", \"explanation\": \"\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"\", \"choices\": [], \"answer\": \"A\", \"explanation\": \"\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"\", \"choices\": [], \"answer\": \"A\", \"explanation\": \"\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"\", \"choices\": [], \"answer\": \"A\", \"explanation\": \"\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"\", \"choices\": [], \"answer\": \"A\", \"explanation\": \"\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"\", \"choices\": [], \"answer\": \"A\", \"explanation\": \"\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"\", \"choices\": [], \"answer\": \"A\", \"explanation\": \"\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"\", \"choices\": [], \"answer\": \"A\", \"explanation\": \"\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"\", \"choices\": [], \"answer\": \"A\", \"explanation\": \"\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"\", \"choices\": [], \"answer\": \"A\", \"explanation\": \"\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"\", \"choices\": [], \"answer\": \"A\", \"explanation\": \"\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"\", \"choices\": [], \"answer\": \"A\", \"explanation\": \"\"}{\"topic\": \"\n",
      "{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"P9K, Q8L, R7M, S6N, __?\", \"choices\": [\"A) A) T5O\", \"B) B) U4P\", \"C) C) V3R\", \"D) D) W2S\"], \"answer\": \"\", \"explanation\": \"Step-by-step analysis reveals...\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"F1G, G2H, H3I, I4J, ?\", \"choices\": [\"A) A) J5K\", \"B) B) K6L\", \"C) C) M7N\", \"D) D) P8Q\"], \"answer\": \"\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"X9Y, W8Z, V7A, ?, ?\"} {\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"??\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\", \"question\": \"?\"}{\"topic\":\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "Time taken per batch generation: [27.761030197143555, 18.46584177017212]\n",
      "Tokens generated per batch: [5120, 5120]\n",
      "Total Time Taken: 46.227 seconds; Total Tokens: 10240; TGPS: 221.516 seconds\n",
      "\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "Skipping invalid JSON at index 0: {\"!topic\":\"Logical Reasoning/Syllogisms\",\"!question\":\"Statements:\\nI. Some books are novels.\\nII. All novels are stories.\\nIII. Some stories are poems.\\n\\nConclusion I: Some poems are books.\\nConclusion II: Some stories are novels.\",\"!choices\":[\"A) A) Only Conclusion I follows\",\"B) B) Only Conclusion II follows\",\"C) C) Both I and II follow\",\"D) D) Neither I nor II follows\"],\"!answer\":\"D\",\"!explanation\"} {\"topic\": \"Logical Reasoning/Syllogisms\", \"question\": \"Statements:\\nI. Some books are novels.\\nII. All novels are stories.\\nIII. Some stories are poems.\", \"choices\": [\"A) A) If only conclusion I follows\", \"B) B) If only conclusion II follows\", \"C) C) If conclusions I and II both follow\", \"D) D) If neither conclusion I nor conclusion II follows\"], \"answer\": \"\", \"explanation\": \"\"}{\"!\"}{\"topic\": \"Logical Reasoning/Syllogisms\", \"question\": \"Statements:\\nI. Some books are novels.\\nII. All novels are stories.\\nIII. Some stories are poems.\\n\\nConclusion I: Some poems are books.\\nConclusion II: Some stories are novels.\"}{\"!\"}{\"topic\": \"Logical Reasoning/Syllogisms\", \"question\": \"Statements:\\nI. Some books are novels.\\nII. All novels are stories.\\nIII. Some stories are poems.\\n\\nConclusion I: Some poems are books.\\nConclusion II: Some stories are novels.\"}{\"!\"}{\"topic\": \"Logical Reasoning/Syllogisms\", \"question\": \"Statements:\\nI. Some books are novels.\\nII. All novels are stories.\\nIII. Some stories are poems.\\n\\nConclusion I: Some poems are books.\\nConclusion II: Some stories are novels.\"}{\"!\"}{\"topic\": \"Logical Reasoning/Syllogisms\", \"question\": \"Statements:\\nI. Some books are novels.\\nII. All novels are stories.\\nIII. Some stories are poems.\\n\\nConclusion I: Some poems are books.\\nConclusion II: Some stories are novels.\"}{\"!\"}{\"topic\": \"Logical Reasoning/Syllogisms\", \"question\": \"Statements:\\nI. Some books are novels.\\nII. All novels are stories.\\nIII. Some stories are poems.\\n\\nConclusion I: Some poems are books.\\nConclusion II: Some stories are novels.\"}{\"!\"}{\"topic\": \"Logical Reasoning/Syllogisms\", \"question\": \"Statements:\\nI. Some books are novels.\\nII. All novels are stories.\\nIII. Some stories are poems.\\n\\nConclusion I: Some poems are books.\\nConclusion II: Some stories are novels.\"}{\"!\"}{\"topic\": \"Logical Reasoning/Syllogisms\", \"question\": \"Statements:\\nI. Some books are novels.\\nII. All novels are stories.\\nIII. Some stories are poems.\\n\\nConclusion I: Some poems are books.\\nConclusion II: Some stories are novels.\"}{\"!\"}{\"topic\": \"Logical Reasoning/Syllogisms\", \"question\": \"Statements:\\nI. Some books are novels.\\nII. All novels are stories.\\nIII. Some stories are poems.\\n\\nConclusion I: Some poems are books.\\nConclusion II: Some stories are novels.\"}{\"!\"}{\"topic\": \"Logical Reasoning/Syllogisms\", \"question\": \"Statements:\\nI. Some books are novels.\\nII. All novels are stories.\\nIII. Some stories are poems.\\n\\nConclusion I: Some poems are books.\\nConclusion II: Some stories are novels.\"}{\"!\"}{\"topic\": \"Logical Reasoning/Syllogisms\", \"question\": \"Statements:\\nI. Some books are novels.\\nII. All novels are stories.\\nIII. Some stories are poems.\\n\\nConclusion I: Some poems are books.\\nConclusion II: Some stories are novels.\"}{\"!\"}{\"topic\": \"Logical Reasoning/Syllogisms\", \"question\": \"Statements:\\nI. Some books are novels.\\nII. All novels are stories.\\nIII. Some stories are poems.\\n\\nConclusion I: Some poems are books.\\nConclusion II: Some stories are novels.\"}{\"!\"}{\"topic\": \"Logical Reasoning/Syllogisms\", \"question\": \"Statements:\\nI. Some books are novels.\\nII. All novels are stories.\\nIII. Some stories are poems.\\n\\nConclusion I: Some poems are books.\\nConclusion II: Some stories are novels.\"}{\"!\"}{\"topic\": \"Logical Reasoning/Syllogisms\", \"question\": \"Statements:\\nI. Some books are novels.\\\n",
      "Saved to outputs/questions.json!\n"
     ]
    }
   ],
   "source": [
    "# ========== TEST Q-AGENT ==========\n",
    "!python -m agents.question_agent \\\n",
    "    --output_file \"outputs/questions.json\" \\\n",
    "    --num_questions 10 \\\n",
    "    --batch_size 5 \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e78669d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw questions: 10\n",
      "Passed filter: 2\n",
      "Pass rate: 20.0%\n",
      "CRITICAL: Below 50% = DISQUALIFIED. Re-train or adjust prompts.\n"
     ]
    }
   ],
   "source": [
    "# ========== CHECK Q-AGENT FILTER PASS RATE ==========\n",
    "import json\n",
    "\n",
    "with open(\"outputs/questions.json\", \"r\") as f:\n",
    "    questions = json.load(f)\n",
    "with open(\"outputs/filtered_questions.json\", \"r\") as f:\n",
    "    filtered = json.load(f)\n",
    "\n",
    "pass_rate = len(filtered) / max(len(questions), 1) * 100\n",
    "print(f\"Raw questions: {len(questions)}\")\n",
    "print(f\"Passed filter: {len(filtered)}\")\n",
    "print(f\"Pass rate: {pass_rate:.1f}%\")\n",
    "if pass_rate < 50:\n",
    "    print(\"CRITICAL: Below 50% = DISQUALIFIED. Re-train or adjust prompts.\")\n",
    "else:\n",
    "    print(\"Filter pass rate OK.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c9496d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:410: UserWarning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (Triggered internally at /app/pytorch/c10/core/AllocatorConfig.cpp:28.)\n",
      "  torch._C._cuda_init()\n",
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "#### Unsloth: `hf_xet==1.1.10` and `ipykernel>6.30.1` breaks progress bars. Disabling for now in XET.\n",
      "#### Unsloth: To re-enable progress bars, please downgrade to `ipykernel==6.30.1` or wait for a fix to\n",
      "https://github.com/huggingface/xet-core/issues/526\n",
      "INFO 02-15 09:25:55 [__init__.py:225] Automatically detected platform rocm.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "/usr/local/lib/python3.12/dist-packages/unsloth_zoo/gradient_checkpointing.py:348: UserWarning: expandable_segments not supported on this platform (Triggered internally at /app/pytorch/c10/hip/HIPAllocatorConfig.h:39.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"{DEVICE_TYPE_TORCH}:{i}\") for i in range(n_gpus)])\n",
      "==((====))==  Unsloth 2025.10.9: Fast Qwen3 patching. Transformers: 4.56.2. vLLM: 0.11.1rc3.dev39+gf417746ad.rocm700.\n",
      "   \\\\   /|    . Num GPUs = 1. Max memory: 255.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0a0+git1c57644. ROCm Toolkit: 7.0.51831-a3e329ad8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "[2026-02-15 09:25:59] INFO modeling.py:987: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:05<00:00,  1.99s/it]\n",
      "STEPS:   0%|                                           | 0/1 [00:00<?, ?batch/s]/usr/local/lib/python3.12/dist-packages/unsloth/kernels/utils.py:743: UserWarning: An output with one or more elements was resized since it had shape [1, 2, 2560], which does not match the required output shape [2, 1, 2560]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /app/pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  out = torch_matmul(X, W.t(), out = out)\n",
      "STEPS: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:17<00:00, 17.78s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "STEPS: : 2batch [00:26, 13.14s/batch]                                           \n",
      "\n",
      "=== Question 1 ===\n",
      "Question: Ravi says,‚Äù‚ÄòRam is the brother of my father's youngest child.‚Äô Who is Ram to Ravi?\n",
      "Expected: A\n",
      "Model Answer:\n",
      "{\"answer\": \"B\", \"reasoning\": \"Ravi mentions that Ram is the brother of his father's youngest child. Since Ravi has no siblings mentioned, Ram must be the cousin of Ravi.\"}{\"answer\": \"C\", \"reasoning\": \"Since Ram is the brother of Ravi's father's youngest child, he would also be the nephew of Ravi.\"}{\"answer\": \"D\", \"reasoning\": \"If Ram were the stepbrother of Ravi, it would imply they share one parent but not both parents.\"}{\"answer\": \"A\", \"reasoning\": \"This option does not fit because if Ram were the brother of Ravi, they would share at least one parent.\"}{\"answer\": \"B\", \"reasoning\": \"The most accurate relationship based on the given information\"}{\"answer\": \"C\", \"reasoning\": \"Ram is the brother of Ravi's uncle's child, making him the nephew of Ravi.\"}{\"answer\": \"B\", \"reasoning\": \"Ram is the brother of Ravi's father's youngest child, so he is the cousin of Ravi.\"}{\"answer\": \"C\", \"reasoning\": \"Since Ram is the brother of Ravi's father's youngest child, he is the nephew of Ravi.\"}{\"answer\": \"B\", \"reasoning\": \"Based on the statement provided, Ram is the cousin of Ravi.\"}{\"answer\": \"C\", \"reasoning\": \"Ram is the brother of Ravi's father's youngest child, which makes him the nephew of Ravi.\"}{\"answer\": \"B\", \"reasoning\": \"Ram is the brother of Ravi's father's youngest child, hence he is the cousin of Ravi.\"}{\"answer\": \"C\", \"reasoning\": \"As per the given information, Ram is the nephew of Ravi.\"}{\"answer\": \"B\", \"reasoning\": \"Ram is the brother of Ravi's father's youngest child, making him the cousin of Ravi.\"}{\"answer\": \"C\", \"reasoning\": \"Since Ram is the brother of Ravi's father's youngest child, he is the nephew of Ravi.\"}{\"answer\": \"B\", \"reasoning\": \"Ram is the brother of Ravi's father's youngest child, thus he is the cousin of Ravi.\"}{\"answer\": \"C\", \"reasoning\": \"Based on the information given, Ram is the\n",
      "\n",
      "=== Question 2 ===\n",
      "Question: P9K, Q8L, R7M, S6N, __?\n",
      "Expected: \n",
      "Model Answer:\n",
      "{\"answer\": \"B\", \"reasoning\": \"The pattern alternates between letters moving forward in the alphabet and numbers decreasing by one.\"}{\"answer\": \"D\", \"reasoning\": \"Each subsequent term follows a consistent alphabetical progression where both the first character moves forward and the second character moves backward simultaneously with numerical decrement.\"}{\"answer\": \"C\", \"reasoning\": \"This option fits the alternating pattern observed in the sequence\"}{\"answer\": \"A\", \"reasoning\": \"Both characters move forward while the number decreases consistently\"}{\"answer\": \"B\", \"reasoning\": \"It continues the established trend of shifting letters and reducing digits\"}{\"answer\": \"D\", \"reasoning\": \"Continues the pattern of increasing alphabetic position and decreasing numeric value\"}{\"answer\": \"C\", \"reasoning\": \"Maintains consistency with previous terms in terms of positional shifts\"}{\"answer\": \"A\", \"reasoning\": \"Follows the same rule of advancing letters and lowering numbers\"}{\"answer\": \"B\", \"reasoning\": \"Matches the established pattern for both alphabetical and numerical sequences\"}{\"answer\": \"D\", \"reasoning\": \"Consistently adheres to the given rules for letter placement and numerical reduction\"}{\"answer\": \"C\", \"reasoning\": \"Correctly extends the series based on its defined structure\"}{\"answer\": \"B\", \"reasoning\": \"Aligns perfectly with the existing pattern of movement\"}{\"answer\": \"D\", \"reasoning\": \"Complies fully with the specified rules for progression\"}{\"answer\": \"C\", \"reasoning\": \"Extends the sequence accurately according to its underlying logic\"}{\"answer\": \"B\", \"reasoning\": \"Fits seamlessly into the ongoing sequence without deviation\"}{\"answer\": \"D\", \"reasoning\": \"Preserves continuity in both alphabetical advancement and numerical decline\"}{\"answer\": \"C\", \"reasoning\": \"Completes the series correctly based on its established pattern\"}{\"answer\": \"B\", \"reasoning\": \"Ensures smooth continuation of the sequence's characteristics\"}{\"answer\": \"D\", \"reasoning\": \"Accurately reflects the intended direction of change\"}{\"answer\": \"C\", \"reasoning\": \"Maintains coherence throughout the entire sequence\"}{\"answer\": \"B\", \"reasoning\": \"Guarantees uninterrupted flow in accordance with the set rules\"}{\"answer\": \"D\", \"reasonning\": \"Correctly completes the series based on its inherent patterns\"}{\"\n",
      "BATCH - 0\n",
      "Tokens: 1024, Time: 17.778 seconds\n",
      "TGPS: 57.598 seconds\n",
      "BATCH - 1\n",
      "Tokens: 1024, Time: 8.499 seconds\n",
      "TGPS: 120.480 seconds\n",
      "\n",
      "==================================================\n",
      "Total Time: 26.278 seconds; Total Tokens: 2048; TGPS: 77.937 seconds\n"
     ]
    }
   ],
   "source": [
    "# ========== TEST A-AGENT ==========\n",
    "!python -m agents.answer_agent \\\n",
    "    --input_file \"outputs/filtered_questions.json\" \\\n",
    "    --output_file \"outputs/answers.json\" \\\n",
    "    --batch_size 5 \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f0a038a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'outputs/filtered_answers.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33moutputs/filtered_questions.json\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      3\u001b[39m     fq = json.load(f)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutputs/filtered_answers.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      5\u001b[39m     fa = json.load(f)\n\u001b[32m      7\u001b[39m N = \u001b[38;5;28mlen\u001b[39m(fq)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'outputs/filtered_answers.json'"
     ]
    }
   ],
   "source": [
    "# ========== CALCULATE SCORES ==========\n",
    "with open(\"outputs/filtered_questions.json\", \"r\") as f:\n",
    "    fq = json.load(f)\n",
    "with open(\"outputs/filtered_answers.json\", \"r\") as f:\n",
    "    fa = json.load(f)\n",
    "\n",
    "N = len(fq)\n",
    "correct = 0\n",
    "for q, a in zip(fq, fa):\n",
    "    if a is not None and q.get('answer', '')[0].upper() == a.get('answer', '').upper():\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct * 100 / max(N, 1)\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Questions: {N}\")\n",
    "print(f\"Correct answers: {correct}\")\n",
    "print(f\"A-Agent accuracy: {accuracy:.1f}%\")\n",
    "print(f\"Q-Agent score (if opponent had same accuracy): {100-accuracy:.1f}%\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1ed8e8",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 5: Push to GitHub\n",
    "Push code (NOT `hf_models/`) to GitHub before deadline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be5e98f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'child' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/IPython/utils/_process_posix.py:125\u001b[39m, in \u001b[36mProcessHandler.system\u001b[39m\u001b[34m(self, cmd)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     child = \u001b[43mpexpect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m-c\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Vanilla Pexpect\u001b[39;00m\n\u001b[32m    126\u001b[39m flush = sys.stdout.flush\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/pexpect/pty_spawn.py:205\u001b[39m, in \u001b[36mspawn.__init__\u001b[39m\u001b[34m(self, command, args, timeout, maxread, searchwindowsize, logfile, cwd, env, ignore_sighup, echo, preexec_fn, encoding, codec_errors, dimensions, use_poll)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimensions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28mself\u001b[39m.use_poll = use_poll\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/pexpect/pty_spawn.py:303\u001b[39m, in \u001b[36mspawn._spawn\u001b[39m\u001b[34m(self, command, args, preexec_fn, dimensions)\u001b[39m\n\u001b[32m    300\u001b[39m     \u001b[38;5;28mself\u001b[39m.args = [a \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m a.encode(\u001b[38;5;28mself\u001b[39m.encoding)\n\u001b[32m    301\u001b[39m                  \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args]\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m \u001b[38;5;28mself\u001b[39m.ptyproc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spawnpty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[38;5;28mself\u001b[39m.pid = \u001b[38;5;28mself\u001b[39m.ptyproc.pid\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/pexpect/pty_spawn.py:315\u001b[39m, in \u001b[36mspawn._spawnpty\u001b[39m\u001b[34m(self, args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''Spawn a pty and return an instance of PtyProcess.'''\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mptyprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPtyProcess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/ptyprocess/ptyprocess.py:315\u001b[39m, in \u001b[36mPtyProcess.spawn\u001b[39m\u001b[34m(cls, argv, cwd, env, echo, preexec_fn, dimensions, pass_fds)\u001b[39m\n\u001b[32m    314\u001b[39m os.close(exec_err_pipe_write)\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m exec_err_data = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexec_err_pipe_read\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4096\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m os.close(exec_err_pipe_read)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbash git.sh\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/ipykernel/zmqshell.py:740\u001b[39m, in \u001b[36mZMQInteractiveShell.system_piped\u001b[39m\u001b[34m(self, cmd)\u001b[39m\n\u001b[32m    738\u001b[39m         \u001b[38;5;28mself\u001b[39m.user_ns[\u001b[33m\"\u001b[39m\u001b[33m_exit_code\u001b[39m\u001b[33m\"\u001b[39m] = system(cmd)\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28mself\u001b[39m.user_ns[\u001b[33m\"\u001b[39m\u001b[33m_exit_code\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/IPython/utils/_process_posix.py:141\u001b[39m, in \u001b[36mProcessHandler.system\u001b[39m\u001b[34m(self, cmd)\u001b[39m\n\u001b[32m    136\u001b[39m         out_size = \u001b[38;5;28mlen\u001b[39m(child.before)\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m    138\u001b[39m     \u001b[38;5;66;03m# We need to send ^C to the process.  The ascii code for '^C' is 3\u001b[39;00m\n\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# (the character is known as ETX for 'End of Text', see\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# curses.ascii.ETX).\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[43mchild\u001b[49m.sendline(\u001b[38;5;28mchr\u001b[39m(\u001b[32m3\u001b[39m))\n\u001b[32m    142\u001b[39m     \u001b[38;5;66;03m# Read and print any more output the program might produce on its\u001b[39;00m\n\u001b[32m    143\u001b[39m     \u001b[38;5;66;03m# way out.\u001b[39;00m\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'child' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "!bash git.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0643ce21-58e3-4cc9-a717-2efd0fbffa4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
